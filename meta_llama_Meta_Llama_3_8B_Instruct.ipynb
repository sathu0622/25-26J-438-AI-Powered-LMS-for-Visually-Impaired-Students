{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sathu0622/25-26J-438-AI-Powered-LMS-for-Visually-Impaired-Students/blob/AI-Powered-Braille-to-Text-Conversion-and-Automated-Evaluation-System-for-O%2FL-History-Examinations/meta_llama_Meta_Llama_3_8B_Instruct.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "#  STEP 1 ‚Äî Install Dependencies\n",
        "# ============================================================\n",
        "!pip install -q transformers accelerate bitsandbytes peft datasets sentencepiece openpyxl scikit-learn\n",
        "\n",
        "# ============================================================\n",
        "#  STEP 2 ‚Äî Mount Google Drive & Clear Memory\n",
        "# ============================================================\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Clear any cached memory\n",
        "import gc\n",
        "import torch\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "print(\"‚úÖ Memory cleared\")\n",
        "\n",
        "# ============================================================\n",
        "#  STEP 3 ‚Äî Import Libraries\n",
        "# ============================================================\n",
        "import pandas as pd\n",
        "import json\n",
        "import torch\n",
        "import numpy as np\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from datasets import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from huggingface_hub import login\n",
        "\n",
        "print(\"‚úÖ All libraries imported successfully\")\n",
        "\n",
        "# ============================================================\n",
        "#  STEP 3.5 ‚Äî Hugging Face Authentication\n",
        "# ============================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üîê HUGGING FACE AUTHENTICATION REQUIRED\")\n",
        "print(\"=\"*60)\n",
        "print(\"Llama 3.1 is a gated model. You need to:\")\n",
        "print(\"1. Go to: https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct\")\n",
        "print(\"2. Click 'Request Access' and accept the terms\")\n",
        "print(\"3. Create a token at: https://huggingface.co/settings/tokens\")\n",
        "print(\"4. Enter your token below\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "# Get token from user\n",
        "from getpass import getpass\n",
        "hf_token = getpass(\"Enter your Hugging Face token (input will be hidden): \")\n",
        "\n",
        "# Login to Hugging Face\n",
        "try:\n",
        "    login(token=hf_token, add_to_git_credential=True)\n",
        "    print(\"‚úÖ Successfully authenticated with Hugging Face!\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Authentication failed: {str(e)}\")\n",
        "    print(\"\\nPlease make sure:\")\n",
        "    print(\"1. You've requested access to Llama 3.1 model\")\n",
        "    print(\"2. Your access has been approved (check your email)\")\n",
        "    print(\"3. Your token has 'read' permissions\")\n",
        "    raise\n",
        "\n",
        "# ============================================================\n",
        "#  STEP 4 ‚Äî Load Excel Dataset from Google Drive\n",
        "# ============================================================\n",
        "dataset_path = \"/content/drive/MyDrive/Model/Final.xlsx\"\n",
        "\n",
        "try:\n",
        "    df = pd.read_excel(dataset_path)\n",
        "\n",
        "    # Check and standardize column names\n",
        "    df.columns = df.columns.str.strip()  # Remove any whitespace\n",
        "\n",
        "    # Handle different possible column names\n",
        "    column_mapping = {}\n",
        "    for col in df.columns:\n",
        "        col_lower = col.lower()\n",
        "        if 'question' in col_lower:\n",
        "            column_mapping[col] = 'question'\n",
        "        elif 'answer' in col_lower:\n",
        "            column_mapping[col] = 'answer'\n",
        "\n",
        "    df = df.rename(columns=column_mapping)\n",
        "\n",
        "    # Verify required columns exist\n",
        "    if 'question' not in df.columns or 'answer' not in df.columns:\n",
        "        raise ValueError(f\"Required columns not found. Found columns: {list(df.columns)}\")\n",
        "\n",
        "    print(f\"‚úÖ Loaded {len(df)} questions from dataset\")\n",
        "    print(f\"‚úÖ Columns: {list(df.columns)}\")\n",
        "    print(\"\\nFirst 5 rows:\")\n",
        "    print(df.head())\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"‚ùå Error: Dataset file not found!\")\n",
        "    print(f\"Please ensure your Excel file is at: {dataset_path}\")\n",
        "    print(\"Expected columns: 'question' and 'answer'\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error loading dataset: {str(e)}\")\n",
        "    print(f\"Columns found: {list(df.columns) if 'df' in locals() else 'Unable to read file'}\")\n",
        "\n",
        "# ============================================================\n",
        "#  STEP 5 ‚Äî Format Dataset for Llama 3.1 Training\n",
        "# ============================================================\n",
        "def format_instruction(question, answer):\n",
        "    \"\"\"Format in Llama 3.1 Instruct chat template\"\"\"\n",
        "    return f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "\n",
        "You are an expert in Sri Lankan O/L History. Provide accurate and concise answers to history questions.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "{question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\n",
        "{answer}<|eot_id|>\"\"\"\n",
        "\n",
        "# Create formatted training data\n",
        "formatted_data = []\n",
        "for _, row in df.iterrows():\n",
        "    formatted_text = format_instruction(row[\"question\"], row[\"answer\"])\n",
        "    formatted_data.append({\n",
        "        \"text\": formatted_text\n",
        "    })\n",
        "\n",
        "# Convert to Hugging Face Dataset\n",
        "train_dataset = Dataset.from_pandas(pd.DataFrame(formatted_data))\n",
        "print(f\"‚úÖ Formatted {len(train_dataset)} training examples\")\n",
        "\n",
        "# ============================================================\n",
        "#  STEP 6 ‚Äî Load Llama 3.1 8B Model with 4-bit Quantization\n",
        "# ============================================================\n",
        "model_name = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
        "\n",
        "print(\"\\nüîÑ Loading tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_name,\n",
        "    use_fast=True,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# Set padding token\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "tokenizer.padding_side = \"right\"\n",
        "print(\"‚úÖ Tokenizer loaded\")\n",
        "\n",
        "print(\"\\nüîÑ Loading model with 4-bit quantization...\")\n",
        "\n",
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "# Configure 4-bit quantization\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# Prepare model for k-bit training\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "print(\"‚úÖ Model loaded successfully\")\n",
        "\n",
        "# ============================================================\n",
        "#  STEP 7 ‚Äî Configure LoRA for Efficient Fine-tuning\n",
        "# ============================================================\n",
        "lora_config = LoraConfig(\n",
        "    r=16,                                    # LoRA rank\n",
        "    lora_alpha=32,                           # LoRA alpha scaling\n",
        "    target_modules=[                         # Target attention modules\n",
        "        \"q_proj\",\n",
        "        \"k_proj\",\n",
        "        \"v_proj\",\n",
        "        \"o_proj\",\n",
        "        \"gate_proj\",\n",
        "        \"up_proj\",\n",
        "        \"down_proj\"\n",
        "    ],\n",
        "    lora_dropout=0.05,                       # Dropout for regularization\n",
        "    bias=\"none\",                             # Don't train biases\n",
        "    task_type=\"CAUSAL_LM\"                    # Task type\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "print(\"\\n‚úÖ LoRA configuration applied\")\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# ============================================================\n",
        "#  STEP 8 ‚Äî Tokenize Dataset\n",
        "# ============================================================\n",
        "def tokenize_function(examples):\n",
        "    \"\"\"Tokenize the text data\"\"\"\n",
        "    result = tokenizer(\n",
        "        examples[\"text\"],\n",
        "        truncation=True,\n",
        "        max_length=512,\n",
        "        padding=\"max_length\",\n",
        "        return_tensors=None\n",
        "    )\n",
        "    # Set labels for causal language modeling\n",
        "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "    return result\n",
        "\n",
        "print(\"\\nüîÑ Tokenizing dataset...\")\n",
        "tokenized_dataset = train_dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=train_dataset.column_names,\n",
        "    desc=\"Tokenizing\"\n",
        ")\n",
        "print(\"‚úÖ Dataset tokenized\")\n",
        "\n",
        "# ============================================================\n",
        "#  STEP 9 ‚Äî Create Validation Split\n",
        "# ============================================================\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split dataset: 80% training, 20% validation\n",
        "train_indices, val_indices = train_test_split(\n",
        "    range(len(tokenized_dataset)),\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "train_subset = tokenized_dataset.select(train_indices)\n",
        "val_subset = tokenized_dataset.select(val_indices)\n",
        "\n",
        "print(f\"\\n‚úÖ Dataset split:\")\n",
        "print(f\"   Training samples: {len(train_subset)}\")\n",
        "print(f\"   Validation samples: {len(val_subset)}\")\n",
        "\n",
        "# ============================================================\n",
        "#  STEP 10 ‚Äî Set Up Training Arguments\n",
        "# ============================================================\n",
        "output_dir = \"/content/drive/MyDrive/Model/ol_history_model\"\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    # Output directory\n",
        "    output_dir=output_dir,\n",
        "\n",
        "    # Training parameters\n",
        "    num_train_epochs=3,                      # Number of epochs\n",
        "    per_device_train_batch_size=1,           # Reduced from 2 to 1\n",
        "    per_device_eval_batch_size=1,            # Reduced from 2 to 1\n",
        "    gradient_accumulation_steps=8,           # Increased from 4 to 8\n",
        "\n",
        "    # Optimizer settings\n",
        "    learning_rate=2e-4,                      # Learning rate\n",
        "    weight_decay=0.01,                       # Weight decay\n",
        "    warmup_steps=50,                         # Warmup steps\n",
        "    optim=\"paged_adamw_8bit\",               # 8-bit optimizer\n",
        "\n",
        "    # Evaluation settings\n",
        "    eval_strategy=\"epoch\",                   # Evaluate after each epoch\n",
        "    eval_steps=None,                         # Eval every N steps (None = use strategy)\n",
        "    load_best_model_at_end=True,            # Load best model at end\n",
        "    metric_for_best_model=\"eval_loss\",      # Metric to track\n",
        "    eval_accumulation_steps=4,               # Accumulate eval to save memory\n",
        "\n",
        "    # Logging and saving\n",
        "    logging_steps=20,                        # Increased from 10\n",
        "    save_strategy=\"epoch\",                   # Save after each epoch\n",
        "    save_total_limit=1,                      # Keep only 1 checkpoint (was 2)\n",
        "\n",
        "    # Performance & Memory optimization\n",
        "    fp16=True,                               # Mixed precision training\n",
        "    gradient_checkpointing=True,             # Save memory\n",
        "    max_grad_norm=0.3,                       # Gradient clipping\n",
        "\n",
        "    # Other settings\n",
        "    report_to=\"none\",                        # Don't report to wandb/tensorboard\n",
        "    remove_unused_columns=False,             # Keep all columns\n",
        "    dataloader_pin_memory=False,             # Disable pin memory to save RAM\n",
        ")\n",
        "\n",
        "print(\"\\n‚úÖ Training arguments configured (Memory optimized)\")\n",
        "\n",
        "# ============================================================\n",
        "#  STEP 11 ‚Äî Define Accuracy Metrics\n",
        "# ============================================================\n",
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"\n",
        "    Compute perplexity and accuracy metrics for evaluation\n",
        "    \"\"\"\n",
        "    predictions, labels = eval_pred\n",
        "\n",
        "    # Calculate perplexity from loss\n",
        "    # Perplexity = exp(loss)\n",
        "    loss = np.mean(predictions)\n",
        "    perplexity = np.exp(loss)\n",
        "\n",
        "    return {\n",
        "        \"perplexity\": perplexity,\n",
        "    }\n",
        "\n",
        "print(\"‚úÖ Metrics function defined\")\n",
        "\n",
        "# ============================================================\n",
        "#  STEP 12 ‚Äî Initialize Trainer with Metrics\n",
        "# ============================================================\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False  # We're doing causal LM, not masked LM\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_subset,\n",
        "    eval_dataset=val_subset,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Trainer initialized with validation dataset\")\n",
        "\n",
        "# ============================================================\n",
        "#  STEP 13 ‚Äî Train the Model with Evaluation\n",
        "# ============================================================\n",
        "\n",
        "# Clear memory before training\n",
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "print(\"‚úÖ Memory cleared before training\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üöÄ STARTING TRAINING WITH VALIDATION\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Training samples: {len(train_subset)}\")\n",
        "print(f\"Validation samples: {len(val_subset)}\")\n",
        "print(f\"Epochs: {training_args.num_train_epochs}\")\n",
        "print(f\"Batch size: {training_args.per_device_train_batch_size}\")\n",
        "print(f\"Gradient accumulation: {training_args.gradient_accumulation_steps}\")\n",
        "print(f\"Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "# Start training\n",
        "try:\n",
        "    train_result = trainer.train()\n",
        "    print(\"\\n‚úÖ Training completed!\")\n",
        "except RuntimeError as e:\n",
        "    if \"out of memory\" in str(e):\n",
        "        print(\"\\n‚ùå GPU Out of Memory Error!\")\n",
        "        print(\"\\nüîß Solutions:\")\n",
        "        print(\"1. Restart runtime: Runtime ‚Üí Restart runtime\")\n",
        "        print(\"2. After restart, the code will use even smaller batch size\")\n",
        "        print(\"3. Or reduce max_length in tokenization (line 189) to 256\")\n",
        "        raise\n",
        "    else:\n",
        "        raise\n",
        "\n",
        "# ============================================================\n",
        "#  STEP 14 ‚Äî Display Training Results\n",
        "# ============================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üìä TRAINING RESULTS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Training metrics\n",
        "metrics = train_result.metrics\n",
        "print(f\"Final Training Loss: {metrics.get('train_loss', 'N/A'):.4f}\")\n",
        "print(f\"Training Runtime: {metrics.get('train_runtime', 0):.2f} seconds\")\n",
        "print(f\"Samples per second: {metrics.get('train_samples_per_second', 0):.2f}\")\n",
        "\n",
        "# Get final evaluation metrics\n",
        "print(\"\\nüîç Evaluating on validation set...\")\n",
        "eval_metrics = trainer.evaluate()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üìà VALIDATION RESULTS\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Validation Loss: {eval_metrics.get('eval_loss', 'N/A'):.4f}\")\n",
        "print(f\"Perplexity: {eval_metrics.get('eval_perplexity', 'N/A'):.4f}\")\n",
        "print(f\"Validation Runtime: {eval_metrics.get('eval_runtime', 0):.2f} seconds\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Calculate improvement metrics\n",
        "print(\"\\nüìâ TRAINING PROGRESS:\")\n",
        "if hasattr(trainer.state, 'log_history'):\n",
        "    # Get first and last training loss\n",
        "    train_losses = [log['loss'] for log in trainer.state.log_history if 'loss' in log]\n",
        "    if len(train_losses) >= 2:\n",
        "        initial_loss = train_losses[0]\n",
        "        final_loss = train_losses[-1]\n",
        "        improvement = ((initial_loss - final_loss) / initial_loss) * 100\n",
        "        print(f\"Initial Training Loss: {initial_loss:.4f}\")\n",
        "        print(f\"Final Training Loss: {final_loss:.4f}\")\n",
        "        print(f\"Loss Reduction: {improvement:.2f}%\")\n",
        "\n",
        "    # Get validation losses per epoch\n",
        "    eval_losses = [log['eval_loss'] for log in trainer.state.log_history if 'eval_loss' in log]\n",
        "    if eval_losses:\n",
        "        print(f\"\\nüìä Validation Loss per Epoch:\")\n",
        "        for i, loss in enumerate(eval_losses, 1):\n",
        "            print(f\"   Epoch {i}: {loss:.4f}\")\n",
        "\n",
        "print(\"=\"*60)\n",
        "\n",
        "# ============================================================\n",
        "#  STEP 15 ‚Äî Save the Fine-tuned Model\n",
        "# ============================================================\n",
        "final_model_path = f\"{output_dir}/final_lora_model\"\n",
        "\n",
        "print(f\"\\nüîÑ Saving model to {final_model_path}...\")\n",
        "model.save_pretrained(final_model_path)\n",
        "tokenizer.save_pretrained(final_model_path)\n",
        "\n",
        "# Save training metrics\n",
        "metrics_path = f\"{output_dir}/training_metrics.json\"\n",
        "all_metrics = {\n",
        "    \"train_loss\": metrics.get('train_loss'),\n",
        "    \"train_runtime\": metrics.get('train_runtime'),\n",
        "    \"train_samples_per_second\": metrics.get('train_samples_per_second'),\n",
        "    \"eval_loss\": eval_metrics.get('eval_loss'),\n",
        "    \"eval_perplexity\": eval_metrics.get('eval_perplexity'),\n",
        "    \"eval_runtime\": eval_metrics.get('eval_runtime'),\n",
        "    \"num_train_samples\": len(train_subset),\n",
        "    \"num_val_samples\": len(val_subset),\n",
        "    \"num_epochs\": training_args.num_train_epochs,\n",
        "}\n",
        "\n",
        "import json\n",
        "with open(metrics_path, 'w') as f:\n",
        "    json.dump(all_metrics, f, indent=2)\n",
        "\n",
        "print(f\"‚úÖ Metrics saved to {metrics_path}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"‚úÖ MODEL TRAINING COMPLETE!\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Model saved at: {final_model_path}\")\n",
        "print(f\"Training Loss: {metrics.get('train_loss', 'N/A'):.4f}\")\n",
        "print(f\"Validation Loss: {eval_metrics.get('eval_loss', 'N/A'):.4f}\")\n",
        "print(f\"Perplexity: {eval_metrics.get('eval_perplexity', 'N/A'):.4f}\")\n",
        "print(\"\\nYou can now use this model for evaluation.\")\n",
        "print(\"Next step: Run the evaluation code to test student answers.\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "id": "O6SEUV-XWzOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pH5iviJvxOLh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136,
          "referenced_widgets": [
            "62d165a2cdc9460ca6c314fa0fd64474",
            "9849cfa401174978810dfb256976245a",
            "3544007b10af4f9b8b471ec9018d208b",
            "8b8d611a377f43299bc482ca933add03",
            "eb01e28792a644599d073d03c4045836",
            "4f65cf685a7a44ecacb4216139470485",
            "550862a8e00f4e0983aa6badedceac0e",
            "493c4b1acf31495fbb32ccf5ceeb570e",
            "7a7e52a695e94d8fbcf3384917c95c2c",
            "657b90fc956043eea357a821faf57235",
            "527ac563ad3348d1b0e01d9b7ea798b3"
          ]
        },
        "outputId": "bb648497-d854-4810-87eb-655bea24095b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Enter Hugging Face Token: ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n",
            "‚úÖ Libraries loaded\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "62d165a2cdc9460ca6c314fa0fd64474"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ LLaMA + LoRA model loaded\n",
            "‚úÖ SBERT loaded\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "#  O/L HISTORY ANSWER EVALUATION SYSTEM - FINAL & CORRECTED\n",
        "# ============================================================\n",
        "\n",
        "# =======================\n",
        "# STEP 1 ‚Äî Install libs\n",
        "# =======================\n",
        "!pip install -q sentence-transformers transformers bitsandbytes peft accelerate huggingface_hub scikit-learn\n",
        "\n",
        "# =======================\n",
        "# STEP 2 ‚Äî Mount Drive\n",
        "# =======================\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# =======================\n",
        "# STEP 3 ‚Äî Hugging Face Login\n",
        "# =======================\n",
        "from huggingface_hub import login\n",
        "from getpass import getpass\n",
        "\n",
        "hf_token = getpass(\"Enter Hugging Face Token: \")\n",
        "login(token=hf_token, add_to_git_credential=True)\n",
        "\n",
        "# =======================\n",
        "# STEP 4 ‚Äî Imports\n",
        "# =======================\n",
        "import torch\n",
        "import re\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from peft import PeftModel\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "print(\"‚úÖ Libraries loaded\")\n",
        "\n",
        "# =======================\n",
        "# STEP 5 ‚Äî Model Paths\n",
        "# =======================\n",
        "BASE_MODEL = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
        "LORA_MODEL_PATH = \"/content/drive/MyDrive/Model/ol_history_model/final_lora_model\"\n",
        "\n",
        "# =======================\n",
        "# STEP 6 ‚Äî Quantization\n",
        "# =======================\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")\n",
        "\n",
        "# =======================\n",
        "# STEP 7 ‚Äî Load Model\n",
        "# =======================\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "\n",
        "model = PeftModel.from_pretrained(base_model, LORA_MODEL_PATH)\n",
        "model.eval()\n",
        "\n",
        "print(\"‚úÖ LLaMA + LoRA model loaded\")\n",
        "\n",
        "# =======================\n",
        "# STEP 8 ‚Äî Semantic Model\n",
        "# =======================\n",
        "sbert = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "print(\"‚úÖ SBERT loaded\")\n",
        "\n",
        "# =======================\n",
        "# STEP 9 ‚Äî Generate Model Answer\n",
        "# =======================\n",
        "def generate_correct_answer(question):\n",
        "    \"\"\"\n",
        "    Generate a model answer for a given O/L History question.\n",
        "    \"\"\"\n",
        "\n",
        "    # Set random seed for reproducibility\n",
        "    torch.manual_seed(42)\n",
        "\n",
        "    # Prepare the prompt\n",
        "    prompt = f\"\"\"\n",
        "You are an expert Sri Lankan O/L History teacher.\n",
        "Answer the question clearly and factually in detailed, exam-style narrative suitable for a Grade 11 student.\n",
        "Include all key historical points, but keep language simple.\n",
        "Do NOT add unnecessary commentary or endnotes.\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "    # Tokenize input\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    # Generate output\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=150,   # shorter for exam style\n",
        "            temperature=0.5,      # deterministic output\n",
        "            top_p=1.0,            # consider all words\n",
        "            repetition_penalty=1.1,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    # Decode and clean the response\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response.split(\"Answer:\")[-1].strip()\n",
        "\n",
        "\n",
        "# =======================\n",
        "# STEP 10 ‚Äî Scoring Metrics\n",
        "# =======================\n",
        "def semantic_similarity(correct, student):\n",
        "    emb1 = sbert.encode(correct, convert_to_tensor=True)\n",
        "    emb2 = sbert.encode(student, convert_to_tensor=True)\n",
        "    return round(float(util.cos_sim(emb1, emb2)) * 100, 2)\n",
        "\n",
        "def keyword_overlap_score(correct, student):\n",
        "    vectorizer = TfidfVectorizer(stop_words=\"english\", max_features=20)\n",
        "    try:\n",
        "        tfidf = vectorizer.fit_transform([correct, student])\n",
        "        features = vectorizer.get_feature_names_out()\n",
        "\n",
        "        correct_words = set(features[i] for i,v in enumerate(tfidf[0].toarray()[0]) if v > 0)\n",
        "        student_words = set(features[i] for i,v in enumerate(tfidf[1].toarray()[0]) if v > 0)\n",
        "\n",
        "        if not correct_words:\n",
        "            return 0.0\n",
        "\n",
        "        return round(len(correct_words & student_words) / len(correct_words) * 100, 2)\n",
        "    except:\n",
        "        return 0.0\n",
        "\n",
        "def jaccard_similarity(correct, student):\n",
        "    def tokenize(text):\n",
        "        text = re.sub(r'[^a-z\\s]', '', text.lower())\n",
        "        return set(text.split())\n",
        "\n",
        "    a, b = tokenize(correct), tokenize(student)\n",
        "    if not a or not b:\n",
        "        return 0.0\n",
        "\n",
        "    return round(len(a & b) / len(a | b) * 100, 2)\n",
        "\n",
        "def length_penalty(correct, student):\n",
        "    r = len(student.split()) / max(len(correct.split()), 1)\n",
        "    if 0.5 <= r <= 1.5:\n",
        "        return 1.0\n",
        "    elif r < 0.5:\n",
        "        return 0.8\n",
        "    return 0.9\n",
        "\n",
        "# =======================\n",
        "# STEP 11 ‚Äî Error Detection\n",
        "# =======================\n",
        "HISTORICAL_ERRORS = [\n",
        "    \"british\", \"factory\", \"industrial\", \"ignored agriculture\",\n",
        "    \"little impact\", \"did not contribute\", \"traveling abroad\"\n",
        "]\n",
        "\n",
        "def detect_historical_errors(answer):\n",
        "    errors = sum(1 for e in HISTORICAL_ERRORS if e in answer.lower())\n",
        "    if errors >= 3:\n",
        "        return 0.3\n",
        "    elif errors == 2:\n",
        "        return 0.5\n",
        "    elif errors == 1:\n",
        "        return 0.8\n",
        "    return 1.0\n",
        "\n",
        "# =======================\n",
        "# STEP 12 ‚Äî Final Score\n",
        "# =======================\n",
        "\n",
        "def calculate_final_score(correct, student):\n",
        "    semantic = semantic_similarity(correct, student)\n",
        "    keyword = keyword_overlap_score(correct, student)\n",
        "    jaccard = jaccard_similarity(correct, student)\n",
        "\n",
        "    length_factor = length_penalty(correct, student)\n",
        "    error_factor = detect_historical_errors(student)\n",
        "\n",
        "    # Strict weighted score\n",
        "    final = (semantic * 0.65 + keyword * 0.35) * length_factor * error_factor\n",
        "\n",
        "    if semantic >= 60 and keyword >= 50 and error_factor == 1.0: final += 5\n",
        "\n",
        "    return round(final, 2), semantic, keyword, jaccard, error_factor\n",
        "\n",
        "# =======================\n",
        "# STEP 13 ‚Äî Feedback\n",
        "# =======================\n",
        "def generate_feedback(score):\n",
        "    if score >= 70:\n",
        "        return \"Excellent answer with correct historical understanding.\"\n",
        "    elif score >= 50:\n",
        "        return \"Good answer, but some important points can be improved.\"\n",
        "    elif score >= 40:\n",
        "        return \"Basic understanding shown, but key facts are missing.\"\n",
        "    else:\n",
        "        return \"Incorrect or weak answer. Please revise the lesson.\"\n",
        "\n",
        "# =======================\n",
        "# STEP 14 ‚Äî Evaluation\n",
        "# =======================\n",
        "def evaluate_student_answer(question, student_answer):\n",
        "    correct_answer = generate_correct_answer(question)\n",
        "\n",
        "    final, semantic, keyword, jaccard, error_penalty = calculate_final_score(\n",
        "        correct_answer, student_answer\n",
        "    )\n",
        "\n",
        "    status = \"PASS\" if final >= 60 else \"NEEDS IMPROVEMENT\" if final >= 50 else \"FAIL\"\n",
        "\n",
        "    return {\n",
        "        \"Question\": question,\n",
        "        \"Student Answer\": student_answer,\n",
        "        \"Model Answer\": correct_answer,\n",
        "        \"Final Score (%)\": final,\n",
        "        \"Semantic Similarity (%)\": semantic,\n",
        "        \"Keyword Match (%)\": keyword,\n",
        "        \"Jaccard Similarity (%)\": jaccard,\n",
        "        \"Error Penalty\": f\"{int(error_penalty*100)}%\",\n",
        "        \"Status\": status,\n",
        "        \"Feedback\": generate_feedback(final)\n",
        "    }\n",
        "\n",
        "# =======================\n",
        "# STEP 15 ‚Äî Display Results\n",
        "# =======================\n",
        "def display_results(result):\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"üìä O/L HISTORY ANSWER EVALUATION RESULTS\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    print(\"\\nüìù Question:\")\n",
        "    print(result[\"Question\"])\n",
        "\n",
        "    print(\"\\n‚úçÔ∏è Student Answer:\")\n",
        "    print(result[\"Student Answer\"])\n",
        "\n",
        "    print(\"\\nüìò Model Answer:\")\n",
        "    print(result[\"Model Answer\"])\n",
        "\n",
        "    print(\"\\nüìà Scoring Breakdown:\")\n",
        "    print(f\"   ‚Ä¢ Final Score         : {result['Final Score (%)']}%\")\n",
        "    print(f\"   ‚Ä¢ Semantic Similarity : {result['Semantic Similarity (%)']}%\")\n",
        "    print(f\"   ‚Ä¢ Keyword Match       : {result['Keyword Match (%)']}%\")\n",
        "    print(f\"   ‚Ä¢ Jaccard Similarity  : {result['Jaccard Similarity (%)']}%\")\n",
        "    print(f\"   ‚Ä¢ Error Penalty       : {result['Error Penalty']}\")\n",
        "\n",
        "    print(\"\\nüèÅ Status:\")\n",
        "    print(f\"   ‚Ä¢ {result['Status']}\")\n",
        "\n",
        "    print(\"\\nüí¨ Feedback:\")\n",
        "    print(result[\"Feedback\"])\n",
        "\n",
        "    print(\"=\" * 70 + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =======================\n",
        "# STEP 16 ‚Äî INTERACTIVE MODE\n",
        "# =======================\n",
        "print(\"\\nüéì O/L HISTORY ANSWER EVALUATION SYSTEM\")\n",
        "\n",
        "question = input(\"\\nüìö Enter the question: \").strip()\n",
        "student_answer = input(\"\\n‚úçÔ∏è Enter the student's answer: \").strip()\n",
        "\n",
        "if not question or not student_answer:\n",
        "    print(\"‚ùå Question and answer cannot be empty.\")\n",
        "else:\n",
        "    result = evaluate_student_answer(question, student_answer)\n",
        "    display_results(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6YvbGfqoZqR6",
        "outputId": "e7d7ca72-4b71-417b-958f-75ecafdb58ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üéì O/L HISTORY ANSWER EVALUATION SYSTEM\n",
            "\n",
            "üìö Enter the question: Explain the contributions of King Parakramabahu I to the development of Sri Lanka.\n",
            "\n",
            "‚úçÔ∏è Enter the student's answer: King Parakramabahu I was mainly known for leading wars against the British and building large factories for trade. He ignored agriculture and did not contribute to religion or education. Most of his reign was spent traveling abroad and he had little impact on the development of Sri Lanka.\n",
            "\n",
            "======================================================================\n",
            "üìä O/L HISTORY ANSWER EVALUATION RESULTS\n",
            "======================================================================\n",
            "\n",
            "üìù Question:\n",
            "Explain the contributions of King Parakramabahu I to the development of Sri Lanka.\n",
            "\n",
            "‚úçÔ∏è Student Answer:\n",
            "King Parakramabahu I was mainly known for leading wars against the British and building large factories for trade. He ignored agriculture and did not contribute to religion or education. Most of his reign was spent traveling abroad and he had little impact on the development of Sri Lanka.\n",
            "\n",
            "üìò Model Answer:\n",
            "King Parakramabahu I ruled Anuradhapura during a critical period when the Kalinga Magha invasion had caused widespread destruction. He worked tirelessly to restore the economy, rebuild temples, and revive agriculture. His contributions include:\n",
            "\n",
            "Rebuilding the Galpotha Tank: This large irrigation tank was crucial for supplying water to the dry zone, supporting agriculture, and maintaining food security.\n",
            "\n",
            "Constructing temples: He renovated important Buddhist shrines such as Thirukethishwaram Temple and built new stupas, promoting religious harmony and cultural enrichment.\n",
            "\n",
            "Reviving trade: The king encouraged foreign relations by sending ambassadors to India and strengthening maritime trade networks, which helped restore economic prosperity.\n",
            "\n",
            "Enhancing education: There is evidence that he supported learning institutions\n",
            "\n",
            "üìà Scoring Breakdown:\n",
            "   ‚Ä¢ Final Score         : 15.0%\n",
            "   ‚Ä¢ Semantic Similarity : 74.63%\n",
            "   ‚Ä¢ Keyword Match       : 40.0%\n",
            "   ‚Ä¢ Jaccard Similarity  : 12.82%\n",
            "   ‚Ä¢ Error Penalty       : 30%\n",
            "\n",
            "üèÅ Status:\n",
            "   ‚Ä¢ FAIL\n",
            "\n",
            "üí¨ Feedback:\n",
            "Incorrect or weak answer. Please revise the lesson.\n",
            "======================================================================\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyPQU6Jror3LRgepiGZ+I3Jx",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "62d165a2cdc9460ca6c314fa0fd64474": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9849cfa401174978810dfb256976245a",
              "IPY_MODEL_3544007b10af4f9b8b471ec9018d208b",
              "IPY_MODEL_8b8d611a377f43299bc482ca933add03"
            ],
            "layout": "IPY_MODEL_eb01e28792a644599d073d03c4045836"
          }
        },
        "9849cfa401174978810dfb256976245a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4f65cf685a7a44ecacb4216139470485",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_550862a8e00f4e0983aa6badedceac0e",
            "value": "Loading‚Äácheckpoint‚Äáshards:‚Äá100%"
          }
        },
        "3544007b10af4f9b8b471ec9018d208b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_493c4b1acf31495fbb32ccf5ceeb570e",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7a7e52a695e94d8fbcf3384917c95c2c",
            "value": 4
          }
        },
        "8b8d611a377f43299bc482ca933add03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_657b90fc956043eea357a821faf57235",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_527ac563ad3348d1b0e01d9b7ea798b3",
            "value": "‚Äá4/4‚Äá[01:36&lt;00:00,‚Äá20.06s/it]"
          }
        },
        "eb01e28792a644599d073d03c4045836": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f65cf685a7a44ecacb4216139470485": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "550862a8e00f4e0983aa6badedceac0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "493c4b1acf31495fbb32ccf5ceeb570e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a7e52a695e94d8fbcf3384917c95c2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "657b90fc956043eea357a821faf57235": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "527ac563ad3348d1b0e01d9b7ea798b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}