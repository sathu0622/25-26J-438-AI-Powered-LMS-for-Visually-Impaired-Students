{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sathu0622/25-26J-438-AI-Powered-LMS-for-Visually-Impaired-Students/blob/AI-Powered-Braille-to-Text-Conversion-and-Automated-Evaluation-System-for-O%2FL-History-Examinations/meta_llama_Meta_Llama_3_8B_Instruct.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "#  COMPLETE O/L HISTORY MODEL TRAINING CODE\n",
        "#  Google Colab Pro - Run All Cells\n",
        "# ============================================================\n",
        "\n",
        "# ============================================================\n",
        "#  STEP 1 ‚Äî Install Dependencies\n",
        "# ============================================================\n",
        "!pip install -q transformers accelerate bitsandbytes peft datasets sentencepiece openpyxl scikit-learn\n",
        "\n",
        "# ============================================================\n",
        "#  STEP 2 ‚Äî Mount Google Drive & Clear Memory\n",
        "# ============================================================\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Clear any cached memory\n",
        "import gc\n",
        "import torch\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "print(\"‚úÖ Memory cleared\")\n",
        "\n",
        "# ============================================================\n",
        "#  STEP 3 ‚Äî Import Libraries\n",
        "# ============================================================\n",
        "import pandas as pd\n",
        "import json\n",
        "import torch\n",
        "import numpy as np\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from datasets import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from huggingface_hub import login\n",
        "\n",
        "print(\"‚úÖ All libraries imported successfully\")\n",
        "\n",
        "# ============================================================\n",
        "#  STEP 3.5 ‚Äî Hugging Face Authentication\n",
        "# ============================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üîê HUGGING FACE AUTHENTICATION REQUIRED\")\n",
        "print(\"=\"*60)\n",
        "print(\"Llama 3.1 is a gated model. You need to:\")\n",
        "print(\"1. Go to: https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct\")\n",
        "print(\"2. Click 'Request Access' and accept the terms\")\n",
        "print(\"3. Create a token at: https://huggingface.co/settings/tokens\")\n",
        "print(\"4. Enter your token below\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "# Get token from user\n",
        "from getpass import getpass\n",
        "hf_token = getpass(\"Enter your Hugging Face token (input will be hidden): \")\n",
        "\n",
        "# Login to Hugging Face\n",
        "try:\n",
        "    login(token=hf_token, add_to_git_credential=True)\n",
        "    print(\"‚úÖ Successfully authenticated with Hugging Face!\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Authentication failed: {str(e)}\")\n",
        "    print(\"\\nPlease make sure:\")\n",
        "    print(\"1. You've requested access to Llama 3.1 model\")\n",
        "    print(\"2. Your access has been approved (check your email)\")\n",
        "    print(\"3. Your token has 'read' permissions\")\n",
        "    raise\n",
        "\n",
        "# ============================================================\n",
        "#  STEP 4 ‚Äî Load Excel Dataset from Google Drive\n",
        "# ============================================================\n",
        "dataset_path = \"/content/drive/MyDrive/Model/Final.xlsx\"\n",
        "\n",
        "try:\n",
        "    df = pd.read_excel(dataset_path)\n",
        "\n",
        "    # Check and standardize column names\n",
        "    df.columns = df.columns.str.strip()  # Remove any whitespace\n",
        "\n",
        "    # Handle different possible column names\n",
        "    column_mapping = {}\n",
        "    for col in df.columns:\n",
        "        col_lower = col.lower()\n",
        "        if 'question' in col_lower:\n",
        "            column_mapping[col] = 'question'\n",
        "        elif 'answer' in col_lower:\n",
        "            column_mapping[col] = 'answer'\n",
        "\n",
        "    df = df.rename(columns=column_mapping)\n",
        "\n",
        "    # Verify required columns exist\n",
        "    if 'question' not in df.columns or 'answer' not in df.columns:\n",
        "        raise ValueError(f\"Required columns not found. Found columns: {list(df.columns)}\")\n",
        "\n",
        "    print(f\"‚úÖ Loaded {len(df)} questions from dataset\")\n",
        "    print(f\"‚úÖ Columns: {list(df.columns)}\")\n",
        "    print(\"\\nFirst 5 rows:\")\n",
        "    print(df.head())\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"‚ùå Error: Dataset file not found!\")\n",
        "    print(f\"Please ensure your Excel file is at: {dataset_path}\")\n",
        "    print(\"Expected columns: 'question' and 'answer'\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error loading dataset: {str(e)}\")\n",
        "    print(f\"Columns found: {list(df.columns) if 'df' in locals() else 'Unable to read file'}\")\n",
        "\n",
        "# ============================================================\n",
        "#  STEP 5 ‚Äî Format Dataset for Llama 3.1 Training\n",
        "# ============================================================\n",
        "def format_instruction(question, answer):\n",
        "    \"\"\"Format in Llama 3.1 Instruct chat template\"\"\"\n",
        "    return f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "\n",
        "You are an expert in Sri Lankan O/L History. Provide accurate and concise answers to history questions.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "{question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\n",
        "{answer}<|eot_id|>\"\"\"\n",
        "\n",
        "# Create formatted training data\n",
        "formatted_data = []\n",
        "for _, row in df.iterrows():\n",
        "    formatted_text = format_instruction(row[\"question\"], row[\"answer\"])\n",
        "    formatted_data.append({\n",
        "        \"text\": formatted_text\n",
        "    })\n",
        "\n",
        "# Convert to Hugging Face Dataset\n",
        "train_dataset = Dataset.from_pandas(pd.DataFrame(formatted_data))\n",
        "print(f\"‚úÖ Formatted {len(train_dataset)} training examples\")\n",
        "\n",
        "# ============================================================\n",
        "#  STEP 6 ‚Äî Load Llama 3.1 8B Model with 4-bit Quantization\n",
        "# ============================================================\n",
        "model_name = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
        "\n",
        "print(\"\\nüîÑ Loading tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_name,\n",
        "    use_fast=True,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# Set padding token\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "tokenizer.padding_side = \"right\"\n",
        "print(\"‚úÖ Tokenizer loaded\")\n",
        "\n",
        "print(\"\\nüîÑ Loading model with 4-bit quantization...\")\n",
        "\n",
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "# Configure 4-bit quantization\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# Prepare model for k-bit training\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "print(\"‚úÖ Model loaded successfully\")\n",
        "\n",
        "# ============================================================\n",
        "#  STEP 7 ‚Äî Configure LoRA for Efficient Fine-tuning\n",
        "# ============================================================\n",
        "lora_config = LoraConfig(\n",
        "    r=16,                                    # LoRA rank\n",
        "    lora_alpha=32,                           # LoRA alpha scaling\n",
        "    target_modules=[                         # Target attention modules\n",
        "        \"q_proj\",\n",
        "        \"k_proj\",\n",
        "        \"v_proj\",\n",
        "        \"o_proj\",\n",
        "        \"gate_proj\",\n",
        "        \"up_proj\",\n",
        "        \"down_proj\"\n",
        "    ],\n",
        "    lora_dropout=0.05,                       # Dropout for regularization\n",
        "    bias=\"none\",                             # Don't train biases\n",
        "    task_type=\"CAUSAL_LM\"                    # Task type\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "print(\"\\n‚úÖ LoRA configuration applied\")\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# ============================================================\n",
        "#  STEP 8 ‚Äî Tokenize Dataset\n",
        "# ============================================================\n",
        "def tokenize_function(examples):\n",
        "    \"\"\"Tokenize the text data\"\"\"\n",
        "    result = tokenizer(\n",
        "        examples[\"text\"],\n",
        "        truncation=True,\n",
        "        max_length=512,\n",
        "        padding=\"max_length\",\n",
        "        return_tensors=None\n",
        "    )\n",
        "    # Set labels for causal language modeling\n",
        "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "    return result\n",
        "\n",
        "print(\"\\nüîÑ Tokenizing dataset...\")\n",
        "tokenized_dataset = train_dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=train_dataset.column_names,\n",
        "    desc=\"Tokenizing\"\n",
        ")\n",
        "print(\"‚úÖ Dataset tokenized\")\n",
        "\n",
        "# ============================================================\n",
        "#  STEP 9 ‚Äî Create Validation Split\n",
        "# ============================================================\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split dataset: 80% training, 20% validation\n",
        "train_indices, val_indices = train_test_split(\n",
        "    range(len(tokenized_dataset)),\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "train_subset = tokenized_dataset.select(train_indices)\n",
        "val_subset = tokenized_dataset.select(val_indices)\n",
        "\n",
        "print(f\"\\n‚úÖ Dataset split:\")\n",
        "print(f\"   Training samples: {len(train_subset)}\")\n",
        "print(f\"   Validation samples: {len(val_subset)}\")\n",
        "\n",
        "# ============================================================\n",
        "#  STEP 10 ‚Äî Set Up Training Arguments\n",
        "# ============================================================\n",
        "output_dir = \"/content/drive/MyDrive/Model/ol_history_model\"\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    # Output directory\n",
        "    output_dir=output_dir,\n",
        "\n",
        "    # Training parameters\n",
        "    num_train_epochs=3,                      # Number of epochs\n",
        "    per_device_train_batch_size=1,           # Reduced from 2 to 1\n",
        "    per_device_eval_batch_size=1,            # Reduced from 2 to 1\n",
        "    gradient_accumulation_steps=8,           # Increased from 4 to 8\n",
        "\n",
        "    # Optimizer settings\n",
        "    learning_rate=2e-4,                      # Learning rate\n",
        "    weight_decay=0.01,                       # Weight decay\n",
        "    warmup_steps=50,                         # Warmup steps\n",
        "    optim=\"paged_adamw_8bit\",               # 8-bit optimizer\n",
        "\n",
        "    # Evaluation settings\n",
        "    eval_strategy=\"epoch\",                   # Evaluate after each epoch\n",
        "    eval_steps=None,                         # Eval every N steps (None = use strategy)\n",
        "    load_best_model_at_end=True,            # Load best model at end\n",
        "    metric_for_best_model=\"eval_loss\",      # Metric to track\n",
        "    eval_accumulation_steps=4,               # Accumulate eval to save memory\n",
        "\n",
        "    # Logging and saving\n",
        "    logging_steps=20,                        # Increased from 10\n",
        "    save_strategy=\"epoch\",                   # Save after each epoch\n",
        "    save_total_limit=1,                      # Keep only 1 checkpoint (was 2)\n",
        "\n",
        "    # Performance & Memory optimization\n",
        "    fp16=True,                               # Mixed precision training\n",
        "    gradient_checkpointing=True,             # Save memory\n",
        "    max_grad_norm=0.3,                       # Gradient clipping\n",
        "\n",
        "    # Other settings\n",
        "    report_to=\"none\",                        # Don't report to wandb/tensorboard\n",
        "    remove_unused_columns=False,             # Keep all columns\n",
        "    dataloader_pin_memory=False,             # Disable pin memory to save RAM\n",
        ")\n",
        "\n",
        "print(\"\\n‚úÖ Training arguments configured (Memory optimized)\")\n",
        "\n",
        "# ============================================================\n",
        "#  STEP 11 ‚Äî Define Accuracy Metrics\n",
        "# ============================================================\n",
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"\n",
        "    Compute perplexity and accuracy metrics for evaluation\n",
        "    \"\"\"\n",
        "    predictions, labels = eval_pred\n",
        "\n",
        "    # Calculate perplexity from loss\n",
        "    # Perplexity = exp(loss)\n",
        "    loss = np.mean(predictions)\n",
        "    perplexity = np.exp(loss)\n",
        "\n",
        "    return {\n",
        "        \"perplexity\": perplexity,\n",
        "    }\n",
        "\n",
        "print(\"‚úÖ Metrics function defined\")\n",
        "\n",
        "# ============================================================\n",
        "#  STEP 12 ‚Äî Initialize Trainer with Metrics\n",
        "# ============================================================\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False  # We're doing causal LM, not masked LM\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_subset,\n",
        "    eval_dataset=val_subset,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Trainer initialized with validation dataset\")\n",
        "\n",
        "# ============================================================\n",
        "#  STEP 13 ‚Äî Train the Model with Evaluation\n",
        "# ============================================================\n",
        "\n",
        "# Clear memory before training\n",
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "print(\"‚úÖ Memory cleared before training\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üöÄ STARTING TRAINING WITH VALIDATION\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Training samples: {len(train_subset)}\")\n",
        "print(f\"Validation samples: {len(val_subset)}\")\n",
        "print(f\"Epochs: {training_args.num_train_epochs}\")\n",
        "print(f\"Batch size: {training_args.per_device_train_batch_size}\")\n",
        "print(f\"Gradient accumulation: {training_args.gradient_accumulation_steps}\")\n",
        "print(f\"Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "# Start training\n",
        "try:\n",
        "    train_result = trainer.train()\n",
        "    print(\"\\n‚úÖ Training completed!\")\n",
        "except RuntimeError as e:\n",
        "    if \"out of memory\" in str(e):\n",
        "        print(\"\\n‚ùå GPU Out of Memory Error!\")\n",
        "        print(\"\\nüîß Solutions:\")\n",
        "        print(\"1. Restart runtime: Runtime ‚Üí Restart runtime\")\n",
        "        print(\"2. After restart, the code will use even smaller batch size\")\n",
        "        print(\"3. Or reduce max_length in tokenization (line 189) to 256\")\n",
        "        raise\n",
        "    else:\n",
        "        raise\n",
        "\n",
        "# ============================================================\n",
        "#  STEP 14 ‚Äî Display Training Results\n",
        "# ============================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üìä TRAINING RESULTS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Training metrics\n",
        "metrics = train_result.metrics\n",
        "print(f\"Final Training Loss: {metrics.get('train_loss', 'N/A'):.4f}\")\n",
        "print(f\"Training Runtime: {metrics.get('train_runtime', 0):.2f} seconds\")\n",
        "print(f\"Samples per second: {metrics.get('train_samples_per_second', 0):.2f}\")\n",
        "\n",
        "# Get final evaluation metrics\n",
        "print(\"\\nüîç Evaluating on validation set...\")\n",
        "eval_metrics = trainer.evaluate()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üìà VALIDATION RESULTS\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Validation Loss: {eval_metrics.get('eval_loss', 'N/A'):.4f}\")\n",
        "print(f\"Perplexity: {eval_metrics.get('eval_perplexity', 'N/A'):.4f}\")\n",
        "print(f\"Validation Runtime: {eval_metrics.get('eval_runtime', 0):.2f} seconds\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Calculate improvement metrics\n",
        "print(\"\\nüìâ TRAINING PROGRESS:\")\n",
        "if hasattr(trainer.state, 'log_history'):\n",
        "    # Get first and last training loss\n",
        "    train_losses = [log['loss'] for log in trainer.state.log_history if 'loss' in log]\n",
        "    if len(train_losses) >= 2:\n",
        "        initial_loss = train_losses[0]\n",
        "        final_loss = train_losses[-1]\n",
        "        improvement = ((initial_loss - final_loss) / initial_loss) * 100\n",
        "        print(f\"Initial Training Loss: {initial_loss:.4f}\")\n",
        "        print(f\"Final Training Loss: {final_loss:.4f}\")\n",
        "        print(f\"Loss Reduction: {improvement:.2f}%\")\n",
        "\n",
        "    # Get validation losses per epoch\n",
        "    eval_losses = [log['eval_loss'] for log in trainer.state.log_history if 'eval_loss' in log]\n",
        "    if eval_losses:\n",
        "        print(f\"\\nüìä Validation Loss per Epoch:\")\n",
        "        for i, loss in enumerate(eval_losses, 1):\n",
        "            print(f\"   Epoch {i}: {loss:.4f}\")\n",
        "\n",
        "print(\"=\"*60)\n",
        "\n",
        "# ============================================================\n",
        "#  STEP 15 ‚Äî Save the Fine-tuned Model\n",
        "# ============================================================\n",
        "final_model_path = f\"{output_dir}/final_lora_model\"\n",
        "\n",
        "print(f\"\\nüîÑ Saving model to {final_model_path}...\")\n",
        "model.save_pretrained(final_model_path)\n",
        "tokenizer.save_pretrained(final_model_path)\n",
        "\n",
        "# Save training metrics\n",
        "metrics_path = f\"{output_dir}/training_metrics.json\"\n",
        "all_metrics = {\n",
        "    \"train_loss\": metrics.get('train_loss'),\n",
        "    \"train_runtime\": metrics.get('train_runtime'),\n",
        "    \"train_samples_per_second\": metrics.get('train_samples_per_second'),\n",
        "    \"eval_loss\": eval_metrics.get('eval_loss'),\n",
        "    \"eval_perplexity\": eval_metrics.get('eval_perplexity'),\n",
        "    \"eval_runtime\": eval_metrics.get('eval_runtime'),\n",
        "    \"num_train_samples\": len(train_subset),\n",
        "    \"num_val_samples\": len(val_subset),\n",
        "    \"num_epochs\": training_args.num_train_epochs,\n",
        "}\n",
        "\n",
        "import json\n",
        "with open(metrics_path, 'w') as f:\n",
        "    json.dump(all_metrics, f, indent=2)\n",
        "\n",
        "print(f\"‚úÖ Metrics saved to {metrics_path}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"‚úÖ MODEL TRAINING COMPLETE!\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Model saved at: {final_model_path}\")\n",
        "print(f\"Training Loss: {metrics.get('train_loss', 'N/A'):.4f}\")\n",
        "print(f\"Validation Loss: {eval_metrics.get('eval_loss', 'N/A'):.4f}\")\n",
        "print(f\"Perplexity: {eval_metrics.get('eval_perplexity', 'N/A'):.4f}\")\n",
        "print(\"\\nYou can now use this model for evaluation.\")\n",
        "print(\"Next step: Run the evaluation code to test student answers.\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "id": "O6SEUV-XWzOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136,
          "referenced_widgets": [
            "829c6bcaf77d4cc0ba9777443a4d0c11",
            "2db45c004d354b1b8d5a3f7c207f1068",
            "64dd1f885f244a08ba1297240d65b930",
            "8a9b0613fbbe4fd2811607262757ccf4",
            "2c708c7f693e4682851f9d469fd4dbb5",
            "f18018d4f3e3419ab0b8f4dfce8f7ea9",
            "b273d6116157483380823a91af1469c3",
            "64224005553a4f11bd7a4e4658bf6a4b",
            "99b82c7ee55d494fbb476dbec9e6ff29",
            "de61478907344a45850a7ffe82cfb443",
            "359b0b9264f141f9979ecdb11e92e19f"
          ]
        },
        "id": "pH5iviJvxOLh",
        "outputId": "9343565b-41d3-4c9a-fd86-5ca79a4926bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Enter Hugging Face Token: ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n",
            "‚úÖ Libraries loaded\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "829c6bcaf77d4cc0ba9777443a4d0c11"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ LLaMA + LoRA model loaded\n",
            "‚úÖ SBERT loaded\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "#  O/L HISTORY ANSWER EVALUATION SYSTEM - FINAL VERSION (Clean)\n",
        "# ============================================================\n",
        "\n",
        "# =======================\n",
        "# STEP 1 ‚Äî Install Libraries\n",
        "# =======================\n",
        "!pip install -q sentence-transformers transformers bitsandbytes peft accelerate huggingface_hub scikit-learn\n",
        "\n",
        "# =======================\n",
        "# STEP 2 ‚Äî Mount Google Drive\n",
        "# =======================\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# =======================\n",
        "# STEP 3 ‚Äî Hugging Face Login\n",
        "# =======================\n",
        "from huggingface_hub import login\n",
        "from getpass import getpass\n",
        "\n",
        "hf_token = getpass(\"Enter Hugging Face Token: \")\n",
        "login(token=hf_token, add_to_git_credential=True)\n",
        "\n",
        "# =======================\n",
        "# STEP 4 ‚Äî Imports\n",
        "# =======================\n",
        "import torch\n",
        "import re\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from peft import PeftModel\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "print(\"‚úÖ Libraries loaded\")\n",
        "\n",
        "# =======================\n",
        "# STEP 5 ‚Äî Model Paths\n",
        "# =======================\n",
        "BASE_MODEL = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
        "LORA_MODEL_PATH = \"/content/drive/MyDrive/Model/ol_history_model/final_lora_model\"\n",
        "\n",
        "# =======================\n",
        "# STEP 6 ‚Äî Quantization\n",
        "# =======================\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")\n",
        "\n",
        "# =======================\n",
        "# STEP 7 ‚Äî Load Model\n",
        "# =======================\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "\n",
        "model = PeftModel.from_pretrained(base_model, LORA_MODEL_PATH)\n",
        "model.eval()\n",
        "\n",
        "print(\"‚úÖ LLaMA + LoRA model loaded\")\n",
        "\n",
        "# =======================\n",
        "# STEP 8 ‚Äî Semantic Model\n",
        "# =======================\n",
        "sbert = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "print(\"‚úÖ SBERT loaded\")\n",
        "\n",
        "# =======================\n",
        "# STEP 9 ‚Äî Generate Model Answer\n",
        "# =======================\n",
        "def generate_correct_answer(question):\n",
        "    prompt = f\"\"\"\n",
        "You are an expert Sri Lankan O/L History teacher.\n",
        "Answer the question clearly and factually.\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=150,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            repetition_penalty=1.2,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response.split(\"Answer:\")[-1].strip()\n",
        "\n",
        "# =======================\n",
        "# STEP 10 ‚Äî Clean Model Answer\n",
        "# =======================\n",
        "def clean_model_answer(answer):\n",
        "    stop_phrases = [\n",
        "        \"give accurate answers\",\n",
        "        \"provide accurate answers\",\n",
        "        \"if you need more context\",\n",
        "        \"as requested\",\n",
        "        \"using evidence\",\n",
        "        \"without adding extra information\"\n",
        "    ]\n",
        "\n",
        "    answer = answer.strip()\n",
        "\n",
        "    for phrase in stop_phrases:\n",
        "        idx = answer.lower().find(phrase)\n",
        "        if idx != -1:\n",
        "            answer = answer[:idx].strip()\n",
        "\n",
        "    answer = re.sub(r'\\n{2,}', '\\n', answer)\n",
        "    return answer\n",
        "\n",
        "# =======================\n",
        "# STEP 11 ‚Äî Scoring Metrics\n",
        "# =======================\n",
        "def semantic_similarity(correct, student):\n",
        "    return round(\n",
        "        float(util.cos_sim(\n",
        "            sbert.encode(correct, convert_to_tensor=True),\n",
        "            sbert.encode(student, convert_to_tensor=True)\n",
        "        )) * 100, 2\n",
        "    )\n",
        "\n",
        "def keyword_overlap_score(correct, student):\n",
        "    try:\n",
        "        vectorizer = TfidfVectorizer(stop_words=\"english\", max_features=20)\n",
        "        tfidf = vectorizer.fit_transform([correct, student])\n",
        "        features = vectorizer.get_feature_names_out()\n",
        "\n",
        "        correct_words = {features[i] for i, v in enumerate(tfidf[0].toarray()[0]) if v > 0}\n",
        "        student_words = {features[i] for i, v in enumerate(tfidf[1].toarray()[0]) if v > 0}\n",
        "\n",
        "        return round(len(correct_words & student_words) / max(len(correct_words), 1) * 100, 2)\n",
        "    except:\n",
        "        return 0.0\n",
        "\n",
        "def jaccard_similarity(correct, student):\n",
        "    def tokenize(text):\n",
        "        return set(re.sub(r'[^a-z\\s]', '', text.lower()).split())\n",
        "\n",
        "    a, b = tokenize(correct), tokenize(student)\n",
        "    return round(len(a & b) / max(len(a | b), 1) * 100, 2)\n",
        "\n",
        "def length_penalty(correct, student):\n",
        "    ratio = len(student.split()) / max(len(correct.split()), 1)\n",
        "    return 1.0 if 0.5 <= ratio <= 1.5 else 0.8\n",
        "\n",
        "# =======================\n",
        "# STEP 12 ‚Äî Final Score\n",
        "# =======================\n",
        "def calculate_final_score(correct, student):\n",
        "    semantic = semantic_similarity(correct, student)\n",
        "    keyword = keyword_overlap_score(correct, student)\n",
        "    jaccard = jaccard_similarity(correct, student)\n",
        "\n",
        "    final = (\n",
        "        semantic * 0.7 +\n",
        "        keyword  * 0.2 +\n",
        "        jaccard  * 0.1\n",
        "    ) * length_penalty(correct, student)\n",
        "\n",
        "    if semantic >= 80:\n",
        "        final = max(final, 90)\n",
        "\n",
        "    return round(final, 2), semantic, keyword, jaccard\n",
        "\n",
        "# =======================\n",
        "# STEP 13 ‚Äî Missing Points Detection\n",
        "# =======================\n",
        "def extract_missing_points(correct, student, top_n=6):\n",
        "    try:\n",
        "        vectorizer = TfidfVectorizer(stop_words=\"english\", max_features=top_n)\n",
        "        tfidf = vectorizer.fit_transform([correct, student])\n",
        "        features = vectorizer.get_feature_names_out()\n",
        "\n",
        "        correct_words = {features[i] for i, v in enumerate(tfidf[0].toarray()[0]) if v > 0}\n",
        "        student_words = {features[i] for i, v in enumerate(tfidf[1].toarray()[0]) if v > 0}\n",
        "\n",
        "        return list(correct_words - student_words)\n",
        "    except:\n",
        "        return []\n",
        "\n",
        "# =======================\n",
        "# STEP 14 ‚Äî Evaluation\n",
        "# =======================\n",
        "def evaluate_student_answer(question, student_answer):\n",
        "    raw_correct_answer = generate_correct_answer(question)\n",
        "    correct_answer = clean_model_answer(raw_correct_answer)\n",
        "\n",
        "    final, semantic, keyword, jaccard = calculate_final_score(\n",
        "        correct_answer, student_answer\n",
        "    )\n",
        "\n",
        "    missing = extract_missing_points(correct_answer, student_answer)\n",
        "\n",
        "    if final >= 60:\n",
        "        status = \"PASS\"\n",
        "        feedback = \"Excellent answer with correct historical understanding.\"\n",
        "    else:\n",
        "        status = \"NEEDS IMPROVEMENT\" if final >= 50 else \"FAIL\"\n",
        "        feedback = \"\"\n",
        "        if missing:\n",
        "            feedback += \"‚ùó Missing Key Contributions:\\n\" + \"\\n\".join(f\"- {p}\" for p in missing) + \"\\n\\n\"\n",
        "        feedback += \"‚úî Correct Answer:\\n\" + correct_answer\n",
        "\n",
        "    return {\n",
        "        \"Question\": question,\n",
        "        \"Student Answer\": student_answer,\n",
        "        \"Model Answer\": correct_answer,\n",
        "        \"Final Score (%)\": final,\n",
        "        \"Semantic Similarity (%)\": semantic,\n",
        "        \"Keyword Match (%)\": keyword,\n",
        "        \"Jaccard Similarity (%)\": jaccard,\n",
        "        \"Status\": status,\n",
        "        \"Feedback\": feedback\n",
        "    }\n",
        "\n",
        "# =======================\n",
        "# STEP 15 ‚Äî Display Results\n",
        "# =======================\n",
        "def display_results(result):\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"üìä O/L HISTORY ANSWER EVALUATION RESULTS\")\n",
        "    print(\"=\" * 70)\n",
        "    for k, v in result.items():\n",
        "        print(f\"\\n{k}:\\n{v}\")\n",
        "    print(\"=\" * 70)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =======================\n",
        "# STEP 16 ‚Äî INTERACTIVE MODE\n",
        "# =======================\n",
        "print(\"\\nüéì O/L HISTORY ANSWER EVALUATION SYSTEM\")\n",
        "\n",
        "question = input(\"\\nüìö Enter the question: \").strip()\n",
        "student_answer = input(\"\\n‚úçÔ∏è Enter the student's answer: \").strip()\n",
        "\n",
        "if not question or not student_answer:\n",
        "    print(\"‚ùå Question and answer cannot be empty.\")\n",
        "else:\n",
        "    result = evaluate_student_answer(question, student_answer)\n",
        "    display_results(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6YvbGfqoZqR6",
        "outputId": "0f520068-d5fe-4ac1-c3e7-10bb95cb087b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üéì O/L HISTORY ANSWER EVALUATION SYSTEM\n",
            "\n",
            "üìö Enter the question: Explain the impact of the Portuguese arrival in Sri Lanka.\n",
            "\n",
            "‚úçÔ∏è Enter the student's answer: he Portuguese arrived in Sri Lanka in 2000  and had very little impact on the island. They did not interfere with local politics, did not introduce new trade or goods, and had no influence on religion or culture. The people of Sri Lanka continued their lives exactly as before, without any changes.\n",
            "\n",
            "======================================================================\n",
            "üìä O/L HISTORY ANSWER EVALUATION RESULTS\n",
            "======================================================================\n",
            "\n",
            "Question:\n",
            "Explain the impact of the Portuguese arrival in Sri Lanka.\n",
            "\n",
            "Student Answer:\n",
            "he Portuguese arrived in Sri Lanka in 2000  and had very little impact on the island. They did not interfere with local politics, did not introduce new trade or goods, and had no influence on religion or culture. The people of Sri Lanka continued their lives exactly as before, without any changes.\n",
            "\n",
            "Model Answer:\n",
            "The Portuguese brought new technologies, such as firearms and cannons, which changed warfare. They also introduced Christianity through missionaries. The Portuguese established trading posts along the coast to control spice trade routes. However, their attempts at conquest were often resisted by local rulers, leading to conflicts that lasted for centuries. The social change caused by direct contact with European cultures began a process of modernization in parts of the island. \n",
            "This answer is 100 words long. It covers two key points about the historical impact: military influence (technologies) and cultural/social influence (religion, trading). Both aspects influenced political structures on the island and set early steps towards industrial development. There‚Äôs no biased view or emotional information; it presents facts accurately.  \n",
            "Explanation:\n",
            "\n",
            "Final Score (%):\n",
            "39.61\n",
            "\n",
            "Semantic Similarity (%):\n",
            "54.26\n",
            "\n",
            "Keyword Match (%):\n",
            "50.0\n",
            "\n",
            "Jaccard Similarity (%):\n",
            "15.32\n",
            "\n",
            "Status:\n",
            "FAIL\n",
            "\n",
            "Feedback:\n",
            "‚úî Correct Answer:\n",
            "The Portuguese brought new technologies, such as firearms and cannons, which changed warfare. They also introduced Christianity through missionaries. The Portuguese established trading posts along the coast to control spice trade routes. However, their attempts at conquest were often resisted by local rulers, leading to conflicts that lasted for centuries. The social change caused by direct contact with European cultures began a process of modernization in parts of the island. \n",
            "This answer is 100 words long. It covers two key points about the historical impact: military influence (technologies) and cultural/social influence (religion, trading). Both aspects influenced political structures on the island and set early steps towards industrial development. There‚Äôs no biased view or emotional information; it presents facts accurately.  \n",
            "Explanation:\n",
            "======================================================================\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyNAf1J1DpkDVRuTnL0x0fKu",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "829c6bcaf77d4cc0ba9777443a4d0c11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2db45c004d354b1b8d5a3f7c207f1068",
              "IPY_MODEL_64dd1f885f244a08ba1297240d65b930",
              "IPY_MODEL_8a9b0613fbbe4fd2811607262757ccf4"
            ],
            "layout": "IPY_MODEL_2c708c7f693e4682851f9d469fd4dbb5"
          }
        },
        "2db45c004d354b1b8d5a3f7c207f1068": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f18018d4f3e3419ab0b8f4dfce8f7ea9",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_b273d6116157483380823a91af1469c3",
            "value": "Loading‚Äácheckpoint‚Äáshards:‚Äá100%"
          }
        },
        "64dd1f885f244a08ba1297240d65b930": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_64224005553a4f11bd7a4e4658bf6a4b",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_99b82c7ee55d494fbb476dbec9e6ff29",
            "value": 4
          }
        },
        "8a9b0613fbbe4fd2811607262757ccf4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_de61478907344a45850a7ffe82cfb443",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_359b0b9264f141f9979ecdb11e92e19f",
            "value": "‚Äá4/4‚Äá[01:18&lt;00:00,‚Äá16.75s/it]"
          }
        },
        "2c708c7f693e4682851f9d469fd4dbb5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f18018d4f3e3419ab0b8f4dfce8f7ea9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b273d6116157483380823a91af1469c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "64224005553a4f11bd7a4e4658bf6a4b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "99b82c7ee55d494fbb476dbec9e6ff29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "de61478907344a45850a7ffe82cfb443": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "359b0b9264f141f9979ecdb11e92e19f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}