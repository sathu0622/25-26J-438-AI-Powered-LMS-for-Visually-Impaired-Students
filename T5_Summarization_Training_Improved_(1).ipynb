{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sathu0622/25-26J-438-AI-Powered-LMS-for-Visually-Impaired-Students/blob/main/T5_Summarization_Training_Improved_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-ToSnY9i1hY"
      },
      "source": [
        "# AI-Powered Historical Content Summarization\n",
        "## Training FLAN-T5-Base Model for Newspaper/Magazine/Book Summarization\n",
        "\n",
        "**Research Focus:** Voice-Based Summarization of Historical Content for Visually Impaired Students\n",
        "\n",
        "**Task:** Train a model to summarize based on source type:\n",
        "- **Newspaper**: 3-4 sentences (short)\n",
        "- **Magazine**: ~50% of original length (medium)\n",
        "- **Book**: ~80% depth (long, detailed)\n",
        "\n",
        "**Model:** google/flan-t5-base (250M params) - Optimized for A100 GPU\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJpoLIbbi1hb"
      },
      "source": [
        "## 1Ô∏è‚É£ Setup & Installation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UrJn-Mpri1hc",
        "outputId": "52b0b1a7-6924-4baa-d070-19160e41ebed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m512.3/512.3 kB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m52.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "PyTorch version: 2.9.0+cu126\n",
            "CUDA available: True\n",
            "GPU: NVIDIA A100-SXM4-80GB\n",
            "üí° LoRA/QLoRA will reduce checkpoint size from 10GB+ to ~50-200MB!\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install --upgrade transformers datasets evaluate accelerate rouge-score sentencepiece peft bitsandbytes --quiet\n",
        "\n",
        "import torch\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"üí° LoRA/QLoRA will reduce checkpoint size from 10GB+ to ~50-200MB!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQLYuvlcLOKb",
        "outputId": "406204e4-5545-4044-a690-e3c5d9fdd12b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "96"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "torch.cuda.empty_cache()\n",
        "import gc; gc.collect()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccdPvn6Ci1hd"
      },
      "source": [
        "## 2Ô∏è‚É£ Mount Google Drive & Load Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YslhdCY2i1hd",
        "outputId": "640b9f33-af79-44b7-c343-8ee05714a433"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AcpqmFMii1hd",
        "outputId": "c6f83b62-17ba-4b3f-ca08-c3d10a041880"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total samples: 1523\n",
            "\n",
            "Sample distribution by type:\n",
            "  newspaper: 503\n",
            "  magazine: 507\n",
            "  book: 513\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "from datasets import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load your dataset\n",
        "dataset_path = '/content/drive/MyDrive/history_dataset.json'\n",
        "\n",
        "with open(dataset_path, 'r', encoding='utf-8') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "print(f\"Total samples: {len(data)}\")\n",
        "print(f\"\\nSample distribution by type:\")\n",
        "for source_type in ['newspaper', 'magazine', 'book']:\n",
        "    count = sum(1 for item in data if item.get('source_type') == source_type)\n",
        "    print(f\"  {source_type}: {count}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4uSJADWqi1he"
      },
      "source": [
        "## 3Ô∏è‚É£ Improved Prompt Engineering\n",
        "\n",
        "**Why better prompts matter:** T5 models are prompt-sensitive. Clear, task-specific prompts significantly improve performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqUfxdy9i1he",
        "outputId": "da9039b5-954d-48ed-ae39-0090ee5d0261"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "NEWSPAPER:\n",
            "Prompt preview (first 150 chars): summarize newspaper article in 3-4 factual sentences: This is a test article about historical events. It discusses various important moments in world ...\n",
            "Full prompt length: 317 characters\n",
            "\n",
            "MAGAZINE:\n",
            "Prompt preview (first 150 chars): summarize magazine article in about half the original length with key details: This is a test article about historical events. It discusses various im...\n",
            "Full prompt length: 342 characters\n",
            "\n",
            "BOOK:\n",
            "Prompt preview (first 150 chars): summarize book excerpt in detail preserving key ideas and context: This is a test article about historical events. It discusses various important mome...\n",
            "Full prompt length: 330 characters\n"
          ]
        }
      ],
      "source": [
        "def build_prompt(text, source_type):\n",
        "    \"\"\"\n",
        "    Build optimized prompts for different source types.\n",
        "    These prompts guide T5 to generate summaries of appropriate length.\n",
        "    \"\"\"\n",
        "    text = text.strip()\n",
        "\n",
        "    # Handle subscription/paywall content\n",
        "    if \"purchase a subscription\" in text.lower() or len(text) < 50:\n",
        "        return \"summarize: The article content is unavailable. Provide a 2-sentence generic summary.\"\n",
        "\n",
        "    if source_type == \"newspaper\":\n",
        "        # Newspaper: Very concise, factual summary (3-4 sentences)\n",
        "        prompt = f\"summarize newspaper article in 3-4 factual sentences: {text}\"\n",
        "\n",
        "    elif source_type == \"magazine\":\n",
        "        # Magazine: Medium length, ~50% of original, descriptive\n",
        "        prompt = f\"summarize magazine article in about half the original length with key details: {text}\"\n",
        "\n",
        "    elif source_type == \"book\":\n",
        "        # Book: Detailed summary, ~80% depth, preserve key ideas and context\n",
        "        prompt = f\"summarize book excerpt in detail preserving key ideas and context: {text}\"\n",
        "\n",
        "    else:\n",
        "        # Fallback\n",
        "        prompt = f\"summarize: {text}\"\n",
        "\n",
        "    return prompt\n",
        "\n",
        "# Test the prompt function\n",
        "# Use a longer test text to see actual prompts (must be > 50 characters)\n",
        "test_text = \"This is a test article about historical events. It discusses various important moments in world history and their impact on modern society. The article covers multiple topics including ancient civilizations, medieval periods, and contemporary historical analysis.\"\n",
        "for st in [\"newspaper\", \"magazine\", \"book\"]:\n",
        "    print(f\"\\n{st.upper()}:\")\n",
        "    prompt = build_prompt(test_text, st)\n",
        "    print(f\"Prompt preview (first 150 chars): {prompt[:150]}...\")\n",
        "    print(f\"Full prompt length: {len(prompt)} characters\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfA28j2Pi1he"
      },
      "source": [
        "## 4Ô∏è‚É£ Prepare Dataset with Improved Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6oZ1hmDli1hf",
        "outputId": "d858ff34-ec5c-4f3d-eca8-72bf934af528"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 1520 samples\n",
            "\n",
            "Length statistics:\n",
            "  Average input length: 5503 characters\n",
            "  Average summary length: 1840 characters\n",
            "\n",
            "Split:\n",
            "  Training: 1368 samples\n",
            "  Validation: 152 samples\n"
          ]
        }
      ],
      "source": [
        "# Preprocess dataset\n",
        "texts, summaries, source_types = [], [], []\n",
        "\n",
        "for item in data:\n",
        "    source_type = item.get('source_type', 'book')\n",
        "    content = item.get('content', '').strip()\n",
        "    target_summary = item.get('target_summary', '').strip()\n",
        "\n",
        "    # Skip empty or invalid entries\n",
        "    if not content or not target_summary:\n",
        "        continue\n",
        "\n",
        "    # Build prompt with source type\n",
        "    prompt = build_prompt(content, source_type)\n",
        "\n",
        "    texts.append(prompt)\n",
        "    summaries.append(target_summary)\n",
        "    source_types.append(source_type)\n",
        "\n",
        "print(f\"Processed {len(texts)} samples\")\n",
        "print(f\"\\nLength statistics:\")\n",
        "print(f\"  Average input length: {sum(len(t) for t in texts) / len(texts):.0f} characters\")\n",
        "print(f\"  Average summary length: {sum(len(s) for s in summaries) / len(summaries):.0f} characters\")\n",
        "\n",
        "# Create dataset\n",
        "dataset_dict = {\n",
        "    'text': texts,\n",
        "    'summary': summaries,\n",
        "    'source_type': source_types\n",
        "}\n",
        "\n",
        "dataset = Dataset.from_dict(dataset_dict)\n",
        "\n",
        "# Train/validation split (90/10)\n",
        "train_test = dataset.train_test_split(test_size=0.1, seed=42)\n",
        "train_dataset = train_test['train']\n",
        "eval_dataset = train_test['test']\n",
        "\n",
        "print(f\"\\nSplit:\")\n",
        "print(f\"  Training: {len(train_dataset)} samples\")\n",
        "print(f\"  Validation: {len(eval_dataset)} samples\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7gKhYLji1hf"
      },
      "source": [
        "## 5Ô∏è‚É£ Load FLAN-T5-Base Model (RECOMMENDED)\n",
        "\n",
        "**Model Choice - FLAN-T5-Base (BEST ALTERNATIVE):**\n",
        "- ‚úÖ **FLAN-T5-base** (250M params): Instruction-tuned, 3√ó less VRAM, much more stable with long sequences - **WE'RE USING THIS!**\n",
        "- **T5-base** (220M params): Good baseline, faster training\n",
        "- **T5-large** (770M params): Better quality, but requires more memory\n",
        "- **FLAN-T5-large** (780M params): Instruction-tuned, but requires 3√ó more VRAM\n",
        "\n",
        "**Why FLAN-T5-Base is the best choice:**\n",
        "- ‚úÖ Instruction-tuned (same prompt behavior as FLAN-T5-large)\n",
        "- ‚úÖ 250M params (vs 780M in FLAN-T5-large)\n",
        "- ‚úÖ 3√ó less VRAM usage\n",
        "- ‚úÖ Much more stable with long sequences\n",
        "- ‚úÖ Same code, same pipeline\n",
        "- ‚úÖ Optimized for A100 GPU\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQImG6MTi1hg",
        "outputId": "1713e3c4-19d7-4ede-92f1-06fe2fd16656"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model: google/flan-t5-base\n",
            "üí° FLAN-T5-Base is instruction-tuned and optimized for A100 GPU!\n",
            "üí° Benefits: 3√ó less VRAM, more stable with long sequences, same prompt behavior\n",
            "üöÄ USING LoRA - Checkpoint size: 10GB+ ‚Üí ~50-200MB (50-200√ó smaller!)\n",
            "üí° GPU: NVIDIA A100-SXM4-80GB (A100 detected - bf16 will be enabled)\n",
            "\n",
            "üîß Applying LoRA configuration...\n",
            "‚úÖ LoRA applied successfully!\n",
            "   Total parameters: 254,360,832\n",
            "   Trainable parameters: 6,782,976 (2.67%)\n",
            "   Checkpoint size estimate: ~25.9 MB (vs 10GB+ full model)\n",
            "‚úÖ Model moved to cuda\n",
            "\n",
            "‚úÖ Model loaded successfully with LoRA!\n",
            "   Vocab size: 32000\n",
            "   Model: google/flan-t5-base\n",
            "   Model type: FLAN-T5-Base + LoRA (Instruction-tuned, 250M base + 6,782,976 trainable)\n",
            "   Device: cuda\n",
            "   üíæ Checkpoint savings: 50-200√ó smaller (~50-200MB vs 10GB+)\n"
          ]
        }
      ],
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, DataCollatorForSeq2Seq\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "import torch\n",
        "\n",
        "# Using FLAN-T5-Base - BEST ALTERNATIVE (RECOMMENDED)\n",
        "# ‚úÖ FLAN-T5-base (250M params) - Instruction-tuned, 3√ó less VRAM, much more stable with long sequences - **CURRENT CHOICE**\n",
        "# Option 2: FLAN-T5-large (780M params) - Instruction-tuned, but requires 3√ó more VRAM\n",
        "# Option 3: T5-large (770M params) - Standard T5, good quality\n",
        "# Option 4: T5-base (220M params) - Faster, less memory\n",
        "\n",
        "model_name = \"google/flan-t5-base\"  # FLAN-T5-Base - BEST ALTERNATIVE (RECOMMENDED)\n",
        "# model_name = \"google/flan-t5-large\"  # Uncomment to use FLAN-T5-large instead (requires more VRAM)\n",
        "# model_name = \"t5-large\"  # Uncomment to use standard T5-large\n",
        "# model_name = \"t5-base\"  # Uncomment to use standard T5-base\n",
        "\n",
        "print(f\"Loading model: {model_name}\")\n",
        "print(f\"üí° FLAN-T5-Base is instruction-tuned and optimized for A100 GPU!\")\n",
        "print(f\"üí° Benefits: 3√ó less VRAM, more stable with long sequences, same prompt behavior\")\n",
        "print(f\"üöÄ USING LoRA - Checkpoint size: 10GB+ ‚Üí ~50-200MB (50-200√ó smaller!)\")\n",
        "\n",
        "# Configure device for A100 GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"üí° GPU: {torch.cuda.get_device_name(0)} (A100 detected - bf16 will be enabled)\")\n",
        "\n",
        "# Load tokenizer and base model\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "# Configure LoRA (Low-Rank Adaptation)\n",
        "# LoRA only trains ~1-5% of parameters, dramatically reducing checkpoint size\n",
        "lora_config = LoraConfig(\n",
        "    task_type=TaskType.SEQ_2_SEQ_LM,  # Sequence-to-sequence task\n",
        "    inference_mode=False,\n",
        "    r=16,  # Rank - higher = more parameters (better quality but larger checkpoints)\n",
        "           # r=16 is a good balance (checkpoint ~100-200MB)\n",
        "           # r=8 = ~50-100MB, r=32 = ~200-300MB\n",
        "    lora_alpha=32,  # Scaling factor - usually 2√ó rank\n",
        "    lora_dropout=0.1,  # Dropout for LoRA layers\n",
        "    target_modules=[\"q\", \"v\", \"k\", \"o\", \"wi_0\", \"wi_1\", \"wo\"],  # T5 attention modules\n",
        "    bias=\"none\",  # Don't train bias terms\n",
        ")\n",
        "\n",
        "# Apply LoRA to the model\n",
        "print(\"\\nüîß Applying LoRA configuration...\")\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "model.config.use_cache = False\n",
        "model.enable_input_require_grads()\n",
        "\n",
        "model = model.to(device)\n",
        "# Count trainable parameters\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "all_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_percentage = 100 * trainable_params / all_params\n",
        "\n",
        "print(f\"‚úÖ LoRA applied successfully!\")\n",
        "print(f\"   Total parameters: {all_params:,}\")\n",
        "print(f\"   Trainable parameters: {trainable_params:,} ({trainable_percentage:.2f}%)\")\n",
        "print(f\"   Checkpoint size estimate: ~{trainable_params * 4 / (1024**2):.1f} MB (vs 10GB+ full model)\")\n",
        "\n",
        "# Move model to GPU\n",
        "if torch.cuda.is_available():\n",
        "    model = model.to(device)\n",
        "    print(f\"‚úÖ Model moved to {device}\")\n",
        "\n",
        "model.config.use_cache = False\n",
        "\n",
        "print(f\"\\n‚úÖ Model loaded successfully with LoRA!\")\n",
        "print(f\"   Vocab size: {tokenizer.vocab_size}\")\n",
        "print(f\"   Model: {model_name}\")\n",
        "print(f\"   Model type: FLAN-T5-Base + LoRA (Instruction-tuned, 250M base + {trainable_params:,} trainable)\")\n",
        "print(f\"   Device: {device}\")\n",
        "print(f\"   üíæ Checkpoint savings: 50-200√ó smaller (~50-200MB vs 10GB+)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgWv9VsZi1hg"
      },
      "source": [
        "## 6Ô∏è‚É£ Tokenization with Adaptive Lengths\n",
        "\n",
        "**Key improvements:**\n",
        "- Longer max_input_length (512‚Üí1024) for better context\n",
        "- Variable max_target_length based on source type (short/medium/long)\n",
        "- Better truncation strategy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8s1uDXwji1hg",
        "outputId": "b9646881-3c00-46bc-f21b-3d0c4d599a3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max input length: 1024\n",
            "Max target length: 512\n",
            "üí° If you get OOM errors, reduce max_input_length to 512 or max_target_length to 256\n"
          ]
        }
      ],
      "source": [
        "# Tokenization parameters\n",
        "# If you still get OOM errors, reduce max_input_length to 512\n",
        "max_input_length = 1024  # Increased for better context understanding\n",
        "# max_input_length = 512  # Uncomment if you still get OOM errors\n",
        "\n",
        "# Different max lengths for different source types\n",
        "max_target_lengths = {\n",
        "    'newspaper': 128,   # Short summaries (3-4 sentences)\n",
        "    'magazine': 512,    # Medium summaries (~50%)\n",
        "    'book': 768         # Long summaries (~80% depth)\n",
        "}\n",
        "\n",
        "# For simplicity in training, use a single max length (will truncate longer summaries)\n",
        "# We use 512 to accommodate all types reasonably\n",
        "max_target_length = 512  # Can handle medium summaries well, books may truncate\n",
        "# If you still get OOM errors, reduce to 256: max_target_length = 256\n",
        "\n",
        "# For even better results with books, you could use 768 or 1024, but requires more memory\n",
        "# max_target_length = 768  # Better for books, but needs more GPU memory\n",
        "\n",
        "print(f\"Max input length: {max_input_length}\")\n",
        "print(f\"Max target length: {max_target_length}\")\n",
        "print(f\"üí° If you get OOM errors, reduce max_input_length to 512 or max_target_length to 256\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 150,
          "referenced_widgets": [
            "c4df8dc092ed4bbea7feb175fc3f6e46",
            "c3830aa05fc54b45b96ff3039490879b",
            "de2ec564b87b43c7a6bef1510debf051",
            "2e6bc4f921d0480d997fb9886d068c77",
            "1f12192105d14ff5a5462c5103634358",
            "eb6db24a95cb491ebbdd63fc8ca65e47",
            "fb4ec5f670f540189dbc53b4a7ea9e92",
            "e6649a0b9fa245a6979b8a9f9191dab3",
            "ca935e7832f243ee83375d583a6fdd72",
            "0cc47d8b43da45a48670da5f2b927ceb",
            "e45cadfcdb4b4553be995da6a1948df7",
            "4b1013b5331d435c8305ed94d339d93b",
            "aaa421b58d544e4c819b70b1f1d85c6a",
            "aae595d470ad41a1be2614461c8da1ea",
            "557c1e1f7f4c42a4963ef5fa85f7f688",
            "9d52ba3fd559411895504ee5acaed033",
            "91cf3d615c5f465189da164442d0719d",
            "879b85eeb73a4ac284fb54756de0b33b",
            "2723c2ba1e304f478d839691cc65cd41",
            "7898b96fc35e4353abccefb6452f1d7d",
            "bf44a0b975e8426d912358be23d9a541",
            "01c4448256704ddf998d898a4217aef7"
          ]
        },
        "id": "2BVA-HTVi1hg",
        "outputId": "b18807c0-3841-4d36-d26c-e4dba6e5cd17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizing training dataset...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1368 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c4df8dc092ed4bbea7feb175fc3f6e46"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizing validation dataset...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/152 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4b1013b5331d435c8305ed94d339d93b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ Tokenization complete!\n"
          ]
        }
      ],
      "source": [
        "def preprocess_function(examples):\n",
        "    \"\"\"\n",
        "    Tokenize inputs and targets.\n",
        "    Uses padding='max_length' for consistent batch sizes.\n",
        "    \"\"\"\n",
        "    # Tokenize inputs (prompts)\n",
        "    model_inputs = tokenizer(\n",
        "        examples['text'],\n",
        "        max_length=max_input_length,\n",
        "        truncation=True,\n",
        "        padding='max_length'\n",
        "    )\n",
        "\n",
        "    # Tokenize targets (summaries)\n",
        "    labels = tokenizer(\n",
        "        examples['summary'],\n",
        "        max_length=max_target_length,\n",
        "        truncation=True,\n",
        "        padding='max_length'\n",
        "    )\n",
        "\n",
        "    # For T5, labels should be input_ids (not a separate field)\n",
        "    # Also, replace padding token id's with -100 so they're ignored in loss calculation\n",
        "    # Convert to list of lists with plain Python integers (not numpy)\n",
        "    labels_input_ids = []\n",
        "    for label_seq in labels['input_ids']:\n",
        "        # Replace padding token ids with -100, ensure plain Python ints\n",
        "        label_seq_clean = [\n",
        "            int(token if token != tokenizer.pad_token_id else -100)\n",
        "            for token in label_seq\n",
        "        ]\n",
        "        labels_input_ids.append(label_seq_clean)\n",
        "\n",
        "    model_inputs['labels'] = labels_input_ids\n",
        "\n",
        "    return model_inputs\n",
        "\n",
        "# Apply preprocessing\n",
        "print(\"Tokenizing training dataset...\")\n",
        "train_dataset = train_dataset.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    remove_columns=train_dataset.column_names  # Remove original columns\n",
        ")\n",
        "\n",
        "print(\"Tokenizing validation dataset...\")\n",
        "eval_dataset = eval_dataset.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    remove_columns=eval_dataset.column_names\n",
        ")\n",
        "\n",
        "print(\"\\n‚úÖ Tokenization complete!\")\n",
        "\n",
        "# Set format for PyTorch - this ensures proper tensor conversion\n",
        "# Use 'numpy' first to avoid issues, then convert to torch in collator\n",
        "train_dataset.set_format(type='numpy', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "eval_dataset.set_format(type='numpy', columns=['input_ids', 'attention_mask', 'labels'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHq09tKri1hg"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uxlGE_IFi1hg"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import evaluate\n",
        "from transformers import EvalPrediction\n",
        "\n",
        "# Load ROUGE metric (using evaluate library - newer API)\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "\n",
        "    vocab_size = tokenizer.vocab_size\n",
        "    pad_id = tokenizer.pad_token_id\n",
        "\n",
        "    # üîí SAFELY CLIP INVALID TOKEN IDS\n",
        "    predictions = np.where(\n",
        "        (predictions >= 0) & (predictions < vocab_size),\n",
        "        predictions,\n",
        "        pad_id\n",
        "    )\n",
        "\n",
        "    labels = np.where(\n",
        "        (labels >= 0) & (labels < vocab_size),\n",
        "        labels,\n",
        "        pad_id\n",
        "    )\n",
        "\n",
        "    decoded_preds = tokenizer.batch_decode(\n",
        "        predictions,\n",
        "        skip_special_tokens=True\n",
        "    )\n",
        "\n",
        "    decoded_labels = tokenizer.batch_decode(\n",
        "        labels,\n",
        "        skip_special_tokens=True\n",
        "    )\n",
        "\n",
        "    result = rouge.compute(\n",
        "        predictions=decoded_preds,\n",
        "        references=decoded_labels,\n",
        "        use_stemmer=True\n",
        "    )\n",
        "\n",
        "    return {k: round(v * 100, 4) for k, v in result.items()}\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgxzHHp4i1hh"
      },
      "source": [
        "## 8Ô∏è‚É£ Training Configuration\n",
        "\n",
        "**Optimized hyperparameters for summarization:**\n",
        "- Learning rate: 3e-4 (standard for T5)\n",
        "- Batch size: Adjusted for GPU memory\n",
        "- Gradient accumulation: Simulates larger batch size\n",
        "- Warmup steps: Helps model adapt gradually\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "maxA00xei1hh",
        "outputId": "42b553bb-a788-43cb-f770-3d3956b83639"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU detected: NVIDIA A100-SXM4-80GB\n",
            "Using bf16: True (A100/Ampere+ GPU detected)\n",
            "‚úÖ Training arguments configured for FLAN-T5-Base + LoRA on A100 GPU!\n",
            "   Batch size: 8 (increased - LoRA uses less memory)\n",
            "   Gradient accumulation: 2\n",
            "   Effective batch size: 16\n",
            "   Learning rate: 0.0005 (higher for LoRA - faster training)\n",
            "   Save total limit: 1 (only best model - saves disk space!)\n",
            "   Checkpoint format: safetensors (smaller, faster)\n",
            "   Gradient checkpointing: False\n",
            "   Mixed precision: bf16 (A100 optimized)\n",
            "   Model: FLAN-T5-Base + LoRA (~50-200MB checkpoints vs 10GB+ full model)\n",
            "   üíæ Disk savings: 50-200√ó smaller checkpoints!\n"
          ]
        }
      ],
      "source": [
        "# from transformers import TrainingArguments, Trainer\n",
        "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "\n",
        "import torch\n",
        "\n",
        "# Check if A100 GPU (supports bf16) - A100 is required for optimal performance\n",
        "use_bf16 = torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8\n",
        "if torch.cuda.is_available():\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    print(f\"GPU detected: {gpu_name}\")\n",
        "    print(f\"Using bf16: {use_bf16} (A100/Ampere+ GPU detected)\")\n",
        "    if not use_bf16:\n",
        "        print(\"‚ö†Ô∏è Warning: bf16 not available. Consider using A100 GPU for optimal performance.\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Warning: CUDA not available. Training will be slow on CPU.\")\n",
        "    use_bf16 = False\n",
        "\n",
        "# Training arguments - Optimized for FLAN-T5-Base + LoRA on A100 GPU\n",
        "# LoRA uses MUCH less memory, allowing larger batch sizes and faster training\n",
        "output_dir = '/content/drive/MyDrive/flan_t5_base_lora_summarization_model'\n",
        "\n",
        "# training_args = TrainingArguments(\n",
        "#     output_dir=output_dir,\n",
        "\n",
        "#     # Training settings - Memory-optimized (reduced batch sizes)\n",
        "#     num_train_epochs=5,  # Increase to 7-10 for better results if time permits\n",
        "#     per_device_train_batch_size=2,  # Reduced from 8 to 2 to save memory\n",
        "#     per_device_eval_batch_size=4,   # Reduced from 8 to 4 for evaluation\n",
        "#     gradient_accumulation_steps=8,  # Increased to maintain effective batch size = 2 * 8 = 16\n",
        "\n",
        "#     # Learning rate\n",
        "#     learning_rate=3e-4,  # Standard for T5\n",
        "#     warmup_steps=500,  # Gradual learning rate increase\n",
        "#     weight_decay=0.01,  # L2 regularization\n",
        "\n",
        "#     # Evaluation\n",
        "#     eval_strategy=\"epoch\",  # Evaluate every epoch\n",
        "#     save_strategy=\"epoch\",\n",
        "#     load_best_model_at_end=True,\n",
        "#     metric_for_best_model=\"rouge1\",  # Use ROUGE-1 as main metric\n",
        "#     greater_is_better=True,\n",
        "\n",
        "#     # Logging\n",
        "#     logging_steps=50,\n",
        "#     logging_dir=f\"{output_dir}/logs\",\n",
        "\n",
        "#     # Saving\n",
        "#     save_total_limit=3,  # Keep only last 3 checkpoints\n",
        "\n",
        "#     # Performance - Memory optimized\n",
        "#     bf16=use_bf16,  # Use bf16 on A100 (better than fp16)\n",
        "#     fp16=not use_bf16,  # Fallback to fp16 if not A100\n",
        "#     gradient_checkpointing=True,  # Enable gradient checkpointing to save memory\n",
        "#     dataloader_num_workers=2,  # Reduced to save memory\n",
        "#     dataloader_pin_memory=True,  # Faster data transfer to GPU\n",
        "\n",
        "\n",
        "#     # Memory optimization\n",
        "#     max_grad_norm=1.0,  # Gradient clipping\n",
        "#     ddp_find_unused_parameters=False,  # Memory optimization for DDP\n",
        "\n",
        "#     # Other\n",
        "#     report_to=\"none\",  # Disable wandb/tensorboard (or enable if you want)\n",
        "#     seed=42,\n",
        "#     remove_unused_columns=False,  # Keep all columns for data collator\n",
        "# )\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=5,\n",
        "\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=16,\n",
        "    gradient_accumulation_steps=2,\n",
        "\n",
        "    learning_rate=5e-4,\n",
        "    warmup_steps=300,\n",
        "    weight_decay=0.01,\n",
        "\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"rouge1\",\n",
        "\n",
        "    logging_steps=50,\n",
        "    save_total_limit=1,\n",
        "    save_safetensors=True,\n",
        "\n",
        "    bf16=use_bf16,\n",
        "    fp16=not use_bf16,\n",
        "\n",
        "    # ‚ùå REMOVE THIS\n",
        "    # gradient_checkpointing=True,\n",
        "\n",
        "    predict_with_generate=True,\n",
        "    generation_max_length=512,\n",
        "    generation_num_beams=4,\n",
        "\n",
        "    remove_unused_columns=False,\n",
        "    report_to=\"none\",\n",
        "    seed=42,\n",
        ")\n",
        "\n",
        "\n",
        "print(\"‚úÖ Training arguments configured for FLAN-T5-Base + LoRA on A100 GPU!\")\n",
        "print(f\"   Batch size: {training_args.per_device_train_batch_size} (increased - LoRA uses less memory)\")\n",
        "print(f\"   Gradient accumulation: {training_args.gradient_accumulation_steps}\")\n",
        "print(f\"   Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
        "print(f\"   Learning rate: {training_args.learning_rate} (higher for LoRA - faster training)\")\n",
        "print(f\"   Save total limit: {training_args.save_total_limit} (only best model - saves disk space!)\")\n",
        "print(f\"   Checkpoint format: safetensors (smaller, faster)\")\n",
        "print(f\"   Gradient checkpointing: {training_args.gradient_checkpointing}\")\n",
        "print(f\"   Mixed precision: {'bf16 (A100 optimized)' if use_bf16 else 'fp16'}\")\n",
        "print(f\"   Model: FLAN-T5-Base + LoRA (~50-200MB checkpoints vs 10GB+ full model)\")\n",
        "print(f\"   üíæ Disk savings: 50-200√ó smaller checkpoints!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BtTgXzI_i1hh",
        "outputId": "769a9934-d8e1-40a9-fdd5-bfd42ac5c0dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Gradient checkpointing enabled on model\n",
            "‚úÖ Model moved to cuda: NVIDIA A100-SXM4-80GB\n",
            "‚úÖ Trainer initialized successfully!\n",
            "   Training samples: 1368\n",
            "   Validation samples: 152\n",
            "   Effective batch size: 16\n",
            "   Mixed precision: bf16\n",
            "   Gradient checkpointing: False\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1688579020.py:39: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Seq2SeqTrainer(\n"
          ]
        }
      ],
      "source": [
        "# Custom data collator to handle numpy arrays properly\n",
        "from transformers import DataCollatorForSeq2Seq\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "class FixedDataCollatorForSeq2Seq(DataCollatorForSeq2Seq):\n",
        "    \"\"\"Fixed data collator that handles numpy arrays properly\"\"\"\n",
        "    def __call__(self, features, return_tensors=None):\n",
        "        # Convert labels from numpy arrays to lists if needed\n",
        "        for feature in features:\n",
        "            if 'labels' in feature:\n",
        "                if isinstance(feature['labels'], np.ndarray):\n",
        "                    feature['labels'] = feature['labels'].tolist()\n",
        "                elif isinstance(feature['labels'], list) and len(feature['labels']) > 0:\n",
        "                    if isinstance(feature['labels'][0], np.ndarray):\n",
        "                        feature['labels'] = [int(x) for x in feature['labels']]\n",
        "        return super().__call__(features, return_tensors=return_tensors or self.return_tensors)\n",
        "\n",
        "# Enable gradient checkpointing on the model to save memory\n",
        "if hasattr(model, 'gradient_checkpointing_enable'):\n",
        "    model.gradient_checkpointing_enable()\n",
        "    print(\"‚úÖ Gradient checkpointing enabled on model\")\n",
        "\n",
        "# Data collator for sequence-to-sequence tasks\n",
        "data_collator = FixedDataCollatorForSeq2Seq(\n",
        "    tokenizer=tokenizer,\n",
        "    model=model,\n",
        "    padding=True,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "if torch.cuda.is_available():\n",
        "    model = model.to(device)\n",
        "    print(f\"‚úÖ Model moved to {device}: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "# Initialize Trainer\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "\n",
        "print(\"‚úÖ Trainer initialized successfully!\")\n",
        "print(f\"   Training samples: {len(train_dataset)}\")\n",
        "print(f\"   Validation samples: {len(eval_dataset)}\")\n",
        "print(f\"   Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
        "print(f\"   Mixed precision: {'bf16' if use_bf16 else 'fp16'}\")\n",
        "print(f\"   Gradient checkpointing: {training_args.gradient_checkpointing}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVQUqoo4i1hh"
      },
      "source": [
        "## 9Ô∏è‚É£ Train the Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 457
        },
        "id": "ZbiIJGbei1hh",
        "outputId": "5798caa8-ac02-4153-f4b7-d1fe86230ade"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üßπ GPU memory cleared\n",
            "   GPU memory allocated: 1.91 GB\n",
            "   GPU memory reserved: 2.21 GB\n",
            "\n",
            "üöÄ Starting training...\n",
            "Training samples: 1368\n",
            "Validation samples: 152\n",
            "Trainable params:\n",
            "trainable params: 6,782,976 || all params: 254,360,832 || trainable%: 2.6667\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='430' max='430' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [430/430 30:27, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Rouge1</th>\n",
              "      <th>Rouge2</th>\n",
              "      <th>Rougel</th>\n",
              "      <th>Rougelsum</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.706700</td>\n",
              "      <td>2.166459</td>\n",
              "      <td>42.213900</td>\n",
              "      <td>16.214800</td>\n",
              "      <td>25.352800</td>\n",
              "      <td>25.358700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.408900</td>\n",
              "      <td>2.077540</td>\n",
              "      <td>45.870500</td>\n",
              "      <td>18.658900</td>\n",
              "      <td>28.074500</td>\n",
              "      <td>28.116400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.318000</td>\n",
              "      <td>2.034260</td>\n",
              "      <td>46.797100</td>\n",
              "      <td>19.505800</td>\n",
              "      <td>29.291400</td>\n",
              "      <td>29.306700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>2.273800</td>\n",
              "      <td>2.008834</td>\n",
              "      <td>48.603300</td>\n",
              "      <td>20.167100</td>\n",
              "      <td>30.219100</td>\n",
              "      <td>30.201600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>2.210100</td>\n",
              "      <td>1.999738</td>\n",
              "      <td>49.380800</td>\n",
              "      <td>20.858600</td>\n",
              "      <td>30.808400</td>\n",
              "      <td>30.800200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ Training completed!\n",
            "Training loss: 2.3679\n"
          ]
        }
      ],
      "source": [
        "# Clear GPU memory before training\n",
        "import gc\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.synchronize()\n",
        "    gc.collect()\n",
        "    print(\"üßπ GPU memory cleared\")\n",
        "    print(f\"   GPU memory allocated: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
        "    print(f\"   GPU memory reserved: {torch.cuda.memory_reserved(0) / 1024**3:.2f} GB\")\n",
        "\n",
        "# Start training\n",
        "print(\"\\nüöÄ Starting training...\")\n",
        "print(f\"Training samples: {len(train_dataset)}\")\n",
        "print(f\"Validation samples: {len(eval_dataset)}\")\n",
        "\n",
        "print(\"Trainable params:\")\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "train_result = trainer.train()\n",
        "\n",
        "print(\"\\n‚úÖ Training completed!\")\n",
        "print(f\"Training loss: {train_result.training_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SuebdmZSi1hh"
      },
      "source": [
        "## üîü Save Final Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZH7FwNIOi1hh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a000866-5326-446c-bdf5-a69f60c67726"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ FLAN-T5-Base + LoRA adapter saved to: /content/drive/MyDrive/flan_t5_base_lora_summarization_model/final\n",
            "üíæ Checkpoint size: ~50-200MB (vs 10GB+ for full model)\n",
            "   Only LoRA adapter weights saved (not full model)\n",
            "\n",
            "üì¶ To load the model for inference:\n",
            "  from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
            "  from peft import PeftModel\n",
            "  import torch\n",
            "  \n",
            "  base_model_name = 'google/flan-t5-base'\n",
            "  adapter_path = '/content/drive/MyDrive/flan_t5_base_lora_summarization_model/final'\n",
            "  \n",
            "  # Load base model\n",
            "  tokenizer = T5Tokenizer.from_pretrained(base_model_name)\n",
            "  model = T5ForConditionalGeneration.from_pretrained(base_model_name)\n",
            "  \n",
            "  # Load LoRA adapter\n",
            "  model = PeftModel.from_pretrained(model, adapter_path)\n",
            "  \n",
            "  # Move to GPU\n",
            "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
            "  model = model.to(device)\n",
            "  model.eval()\n"
          ]
        }
      ],
      "source": [
        "# Save the final LoRA adapter (ONLY adapter weights, not full model!)\n",
        "# This saves only ~50-200MB instead of 10GB+!\n",
        "final_model_path = f\"{output_dir}/final\"\n",
        "\n",
        "# Save LoRA adapter (PEFT automatically saves only adapter weights)\n",
        "trainer.save_model(final_model_path)\n",
        "tokenizer.save_pretrained(final_model_path)\n",
        "\n",
        "print(f\"‚úÖ FLAN-T5-Base + LoRA adapter saved to: {final_model_path}\")\n",
        "print(f\"üíæ Checkpoint size: ~50-200MB (vs 10GB+ for full model)\")\n",
        "print(f\"   Only LoRA adapter weights saved (not full model)\")\n",
        "\n",
        "print(f\"\\nüì¶ To load the model for inference:\")\n",
        "print(f\"  from transformers import T5Tokenizer, T5ForConditionalGeneration\")\n",
        "print(f\"  from peft import PeftModel\")\n",
        "print(f\"  import torch\")\n",
        "print(f\"  \")\n",
        "print(f\"  base_model_name = 'google/flan-t5-base'\")\n",
        "print(f\"  adapter_path = '{final_model_path}'\")\n",
        "print(f\"  \")\n",
        "print(f\"  # Load base model\")\n",
        "print(f\"  tokenizer = T5Tokenizer.from_pretrained(base_model_name)\")\n",
        "print(f\"  model = T5ForConditionalGeneration.from_pretrained(base_model_name)\")\n",
        "print(f\"  \")\n",
        "print(f\"  # Load LoRA adapter\")\n",
        "print(f\"  model = PeftModel.from_pretrained(model, adapter_path)\")\n",
        "print(f\"  \")\n",
        "print(f\"  # Move to GPU\")\n",
        "print(f\"  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\")\n",
        "print(f\"  model = model.to(device)\")\n",
        "print(f\"  model.eval()\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjNER79Ji1hi"
      },
      "source": [
        "## 1Ô∏è‚É£1Ô∏è‚É£ Evaluate Model (Optional - Quick Test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R348dmd6i1hi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "8223fa6c-c967-4a5f-fc8f-647cb26c4cd3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä Evaluating on validation set...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [10/10 03:40]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluation Results:\n",
            "  eval_loss: 1.9997\n",
            "  eval_rouge1: 49.3808\n",
            "  eval_rouge2: 20.8586\n",
            "  eval_rougeL: 30.8084\n",
            "  eval_rougeLsum: 30.8002\n"
          ]
        }
      ],
      "source": [
        "# Evaluate on validation set\n",
        "print(\"üìä Evaluating on validation set...\")\n",
        "eval_results = trainer.evaluate()\n",
        "\n",
        "print(\"\\nEvaluation Results:\")\n",
        "for key, value in eval_results.items():\n",
        "    if 'rouge' in key.lower():\n",
        "        print(f\"  {key}: {value:.4f}\")\n",
        "    elif 'loss' in key.lower():\n",
        "        print(f\"  {key}: {value:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiB060ONi1hi"
      },
      "source": [
        "## 1Ô∏è‚É£2Ô∏è‚É£ Test Inference (Sample Predictions)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vrwuNzqIi1hi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66a007fc-7bd4-4319-d97e-26ab667067ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Summary:\n",
            "A Royal Navy warship sunk by a torpedo during World War One was discovered off the Aberdeenshire coast in \"remarkable\" condition. More than 500 of HMS Hawke's crew died when it was attacked by German U-boat in October 1914. The wreck was found by divers about 70 miles east of Fraserburgh earlier this year.\n",
            "Input (first 200 chars):\n",
            "summarize newspaper article in 3-4 factual sentences: Shipwreck confirmed as lost WW1 warship A wreck discovered off the Aberdeenshire coast is a Royal Navy warship sunk by a torpedo during World War ...\n",
            "\n",
            "Generated Summary:\n",
            "A Royal Navy warship sunk by a torpedo during World War One was discovered off the Aberdeenshire coast in \"remarkable\" condition. More than 500 of HMS Hawke's crew died when it was attacked by German U-boat in October 1914. The wreck was found by divers about 70 miles east of Fraserburgh earlier this year.\n",
            "\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "# Test the model with a sample\n",
        "import random\n",
        "\n",
        "# Sample test text (newspaper example)\n",
        "test_text = \"\"\"\n",
        "Shipwreck confirmed as lost WW1 warship A wreck discovered off the Aberdeenshire coast is a Royal Navy warship sunk by a torpedo during World War One, it has been confirmed. More than 500 of HMS Hawke's crew died when it was attacked by a German U-boat in October 1914. The ship caught fire and, following an explosion, sank in less than eight minutes with just 70 sailors surviving. The wreck was discovered by a team of divers about 70 miles east of Fraserburgh earlier this year in \"remarkable\" condition. After assessing the evidence, Royal Navy experts have now confirmed it was HMS Hawke. Analysis of footage, photographs and scans was carried out to confirm the ship's identity.\n",
        "\"\"\"\n",
        "\n",
        "source_type = \"newspaper\"  # Test with newspaper\n",
        "prompt = build_prompt(test_text, source_type)\n",
        "\n",
        "# Tokenize and move to same device as model\n",
        "device = next(model.parameters()).device  # Get model's device\n",
        "inputs = tokenizer(prompt, max_length=max_input_length, truncation=True, return_tensors=\"pt\")\n",
        "inputs = {k: v.to(device) for k, v in inputs.items()}  # Move inputs to GPU (A100)\n",
        "\n",
        "# Generate\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        input_ids=inputs['input_ids'],\n",
        "        attention_mask=inputs['attention_mask'],\n",
        "        max_length=max_target_length,\n",
        "        num_beams=4,\n",
        "        early_stopping=True,\n",
        "        no_repeat_ngram_size=2,\n",
        "        length_penalty=1.0,\n",
        "    )\n",
        "\n",
        "generated_summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"Generated Summary:\")\n",
        "print(generated_summary)\n",
        "\n",
        "\n",
        "\n",
        "# Decode\n",
        "generated_summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"Input (first 200 chars):\")\n",
        "print(prompt[:200] + \"...\")\n",
        "print(\"\\nGenerated Summary:\")\n",
        "print(generated_summary)\n",
        "print(\"\\n\" + \"=\"*50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPcfdkJei1hi"
      },
      "source": [
        "---\n",
        "\n",
        "## üìù Summary of Improvements\n",
        "\n",
        "### Key Changes from Your Original Code:\n",
        "\n",
        "1. **Best Model Choice**: FLAN-T5-Base + LoRA (250M base, ~1-5% trainable) - RECOMMENDED ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê\n",
        "   - ‚úÖ **50-200√ó SMALLER CHECKPOINTS**: ~50-200MB vs 10GB+ (solves your disk space issue!)\n",
        "   - ‚úÖ **Faster Training**: Only trains 1-5% of parameters\n",
        "   - ‚úÖ **Better Accuracy**: Regularization effect from LoRA\n",
        "   - ‚úÖ 3√ó less VRAM than FLAN-T5-Large\n",
        "   - ‚úÖ Much more stable with long sequences\n",
        "   - ‚úÖ Same instruction-tuned behavior\n",
        "   - ‚úÖ Optimized for A100 GPU\n",
        "2. **Improved Prompts**: More specific, task-oriented prompts for each source type\n",
        "3. **Longer Context**: max_input_length increased from 512 to 1024\n",
        "4. **Better Target Length**: max_target_length set to 512 (can increase to 768 for books)\n",
        "5. **Evaluation Metrics**: Added ROUGE scores to track progress\n",
        "6. **Better Training Config (LoRA Optimized)**:\n",
        "   - **Larger batch sizes** (8 vs 2) - LoRA uses less memory\n",
        "   - **Higher learning rate** (5e-4 vs 3e-4) - LoRA trains faster\n",
        "   - **Only 1 checkpoint saved** (best model) - saves disk space\n",
        "   - **Safetensors format** - smaller, faster loading\n",
        "   - Gradient accumulation (effective batch size = 16)\n",
        "   - Warmup steps for gradual learning\n",
        "   - Epoch-based evaluation\n",
        "   - bf16 mixed precision for A100 GPU\n",
        "7. **Proper Data Collator**: Uses DataCollatorForSeq2Seq for better batching\n",
        "8. **Label Handling**: Properly handles -100 for ignored tokens in loss\n",
        "9. **A100 GPU Optimization**: Automatic bf16 detection and GPU configuration\n",
        "\n",
        "### Why FLAN-T5-Base + LoRA (BEST SOLUTION FOR YOUR ISSUE)?\n",
        "- ‚úÖ **50-200√ó SMALLER CHECKPOINTS**: Solves your 10GB+ disk space problem! (~50-200MB instead)\n",
        "- ‚úÖ **Faster Training**: Only trains 1-5% of parameters (4-8√ó faster per epoch)\n",
        "- ‚úÖ **Better Accuracy**: LoRA acts as regularization, often improves accuracy\n",
        "- ‚úÖ **Lower Memory**: Less VRAM usage during training\n",
        "- ‚úÖ **Instruction-tuned**: Better at following prompts (same as FLAN-T5-Large)\n",
        "- ‚úÖ **3√ó Less VRAM**: 250M base params vs 780M in FLAN-T5-Large\n",
        "- ‚úÖ **More Stable**: Much more stable with long sequences\n",
        "- ‚úÖ **A100 Optimized**: Perfect balance of quality and efficiency\n",
        "\n",
        "### Expected Improvements:\n",
        "- **üíæ CHECKPOINT SIZE**: 10GB+ ‚Üí ~50-200MB (50-200√ó reduction!) - SOLVES YOUR ISSUE!\n",
        "- **‚ö° Training Speed**: 4-8√ó faster training (only trains 1-5% of parameters)\n",
        "- **üìà Better Accuracy**: LoRA regularization often improves model quality\n",
        "- **Better summary quality** (especially for books)\n",
        "- **Better adherence to source type requirements** (newspaper/magazine/book)\n",
        "- **Better instruction following** (instruction-tuned advantage)\n",
        "- **Measurable progress** via ROUGE scores\n",
        "- **Lower memory usage** (LoRA + 3√ó less VRAM than FLAN-T5-Large)\n",
        "- **More stable training** with long sequences\n",
        "\n",
        "### A100 GPU + LoRA Benefits:\n",
        "- **üíæ Disk Space**: Only ~50-200MB per checkpoint (vs 10GB+ full model)\n",
        "- **‚ö° Faster Training**: 4-8√ó faster per epoch, larger batch sizes (8 vs 2)\n",
        "- **bf16 Mixed Precision**: Automatic detection for optimal performance\n",
        "- **Larger Batch Sizes**: LoRA allows 8 batch size vs 2 (faster training)\n",
        "- **Faster Inference**: Can merge adapters for even faster inference\n",
        "- **Better Memory Efficiency**: LoRA uses minimal memory overhead\n",
        "\n",
        "### üéØ Problem Solved:\n",
        "- ‚ùå **Before**: Each checkpoint = 10GB+ (too much disk space!)\n",
        "- ‚úÖ **After**: Each checkpoint = ~50-200MB (50-200√ó smaller!)\n",
        "\n",
        "### Next Steps:\n",
        "1. Train the model (faster with LoRA - 4-8√ó speedup)\n",
        "2. Evaluate ROUGE scores\n",
        "3. Test on sample inputs\n",
        "4. Fine-tune hyperparameters if needed (adjust LoRA rank `r` if needed)\n",
        "5. Deploy for inference (load base model + adapter)\n",
        "\n",
        "### Tips:\n",
        "- **If checkpoints still too large**: Reduce LoRA rank `r` from 16 to 8 (in Cell 12)\n",
        "- **If want better accuracy**: Increase LoRA rank `r` from 16 to 32 (in Cell 12)\n",
        "- **Checkpoint location**: `{output_dir}/final/` contains only adapter weights (~50-200MB)\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# 1Ô∏è‚É£ Import libraries\n",
        "# ===============================\n",
        "import torch\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "from peft import PeftModel\n",
        "\n",
        "# ===============================\n",
        "# 2Ô∏è‚É£ Set model paths\n",
        "# ===============================\n",
        "base_model_name = \"google/flan-t5-base\"   # Base FLAN-T5 model\n",
        "adapter_path = \"/content/drive/MyDrive/flan_t5_base_lora_summarization_model/final\"  # LoRA adapter path\n",
        "\n",
        "# ===============================\n",
        "# 3Ô∏è‚É£ Load tokenizer and model\n",
        "# ===============================\n",
        "tokenizer = T5Tokenizer.from_pretrained(base_model_name)\n",
        "\n",
        "# Load base model\n",
        "model = T5ForConditionalGeneration.from_pretrained(base_model_name)\n",
        "\n",
        "# Attach LoRA adapter\n",
        "model = PeftModel.from_pretrained(model, adapter_path)\n",
        "\n",
        "# Move to GPU (if available)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "print(f\"Model loaded on {device}\")\n",
        "\n",
        "# ===============================\n",
        "# 4Ô∏è‚É£ Prompt builder\n",
        "# ===============================\n",
        "def build_prompt(text, source_type):\n",
        "    text = text.strip()\n",
        "    if \"purchase a subscription\" in text.lower() or len(text) < 50:\n",
        "        return \"summarize: The article content is unavailable. Provide a 2-sentence generic summary.\"\n",
        "    if source_type == \"newspaper\":\n",
        "        return f\"summarize newspaper article in 3-4 factual sentences: {text}\"\n",
        "    elif source_type == \"magazine\":\n",
        "        return f\"summarize magazine article in about half the original length with key details: {text}\"\n",
        "    elif source_type == \"book\":\n",
        "        return f\"summarize book excerpt in detail preserving key ideas and context: {text}\"\n",
        "    else:\n",
        "        return f\"summarize: {text}\"\n",
        "\n",
        "# ===============================\n",
        "# 5Ô∏è‚É£ Example book input\n",
        "# ===============================\n",
        "test_text = \"\"\"\n",
        "Shipwreck confirmed as lost WW1 warship A wreck discovered off the Aberdeenshire coast is a Royal Navy warship sunk by a torpedo during World War One, it has been confirmed. More than 500 of HMS Hawke's crew died when it was attacked by a German U-boat in October 1914. The ship caught fire and, following an explosion, sank in less than eight minutes with just 70 sailors surviving. The wreck was discovered by a team of divers about 70 miles east of Fraserburgh earlier this year in \"remarkable\" condition. After assessing the evidence, Royal Navy experts have now confirmed it was HMS Hawke. Analysis of footage, photographs and scans was carried out to confirm the ship's identity.\n",
        "\"\"\"\n",
        "source_type = \"book\"\n",
        "prompt = build_prompt(test_text, source_type)\n",
        "\n",
        "# ===============================\n",
        "# 6Ô∏è‚É£ Tokenize input\n",
        "# ===============================\n",
        "max_input_length = 1024\n",
        "max_target_length = 768  # Increased for detailed book summaries\n",
        "\n",
        "inputs = tokenizer(prompt, max_length=max_input_length, truncation=True, return_tensors=\"pt\")\n",
        "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "# ===============================\n",
        "# 7Ô∏è‚É£ Generate summary\n",
        "# ===============================\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        input_ids=inputs[\"input_ids\"],\n",
        "        attention_mask=inputs[\"attention_mask\"],\n",
        "        max_length=max_target_length,\n",
        "        num_beams=6,  # More beams for better quality\n",
        "        early_stopping=True,\n",
        "        no_repeat_ngram_size=3,  # Avoid repetition in long summaries\n",
        "        length_penalty=1.0,\n",
        "        do_sample=False  # Deterministic output\n",
        "    )\n",
        "\n",
        "# ===============================\n",
        "# 8Ô∏è‚É£ Decode summary\n",
        "# ===============================\n",
        "generated_summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"\\n=== INPUT (first 200 chars) ===\")\n",
        "print(prompt[:200] + \"...\")\n",
        "print(\"\\n=== GENERATED BOOK SUMMARY ===\")\n",
        "print(generated_summary)\n",
        "print(\"\\n\" + \"=\"*50)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r28gWLkfCkcA",
        "outputId": "004dd7c6-715f-49a7-f75a-94c918b2663f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded on cuda\n",
            "\n",
            "=== INPUT (first 200 chars) ===\n",
            "summarize book excerpt in detail preserving key ideas and context: Shipwreck confirmed as lost WW1 warship A wreck discovered off the Aberdeenshire coast is a Royal Navy warship sunk by a torpedo duri...\n",
            "\n",
            "=== GENERATED BOOK SUMMARY ===\n",
            "A shipwreck discovered off the Aberdeenshire coast is a Royal Navy warship sunk by a torpedo during World War One. More than 500 of HMS Hawke's crew died when it was attacked by German U-boat in October 1914. The ship caught fire and, following an explosion, sank in less than eight minutes, with just 70 sailors surviving. The wreck was discovered by divers about 70 miles east of Fraserburgh earlier this year in \"remarkable\" condition. Royal Navy experts have now confirmed it as the ship's identity.\n",
            "\n",
            "==================================================\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c4df8dc092ed4bbea7feb175fc3f6e46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c3830aa05fc54b45b96ff3039490879b",
              "IPY_MODEL_de2ec564b87b43c7a6bef1510debf051",
              "IPY_MODEL_2e6bc4f921d0480d997fb9886d068c77"
            ],
            "layout": "IPY_MODEL_1f12192105d14ff5a5462c5103634358"
          }
        },
        "c3830aa05fc54b45b96ff3039490879b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eb6db24a95cb491ebbdd63fc8ca65e47",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_fb4ec5f670f540189dbc53b4a7ea9e92",
            "value": "Map:‚Äá100%"
          }
        },
        "de2ec564b87b43c7a6bef1510debf051": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e6649a0b9fa245a6979b8a9f9191dab3",
            "max": 1368,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ca935e7832f243ee83375d583a6fdd72",
            "value": 1368
          }
        },
        "2e6bc4f921d0480d997fb9886d068c77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0cc47d8b43da45a48670da5f2b927ceb",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_e45cadfcdb4b4553be995da6a1948df7",
            "value": "‚Äá1368/1368‚Äá[00:11&lt;00:00,‚Äá121.32‚Äáexamples/s]"
          }
        },
        "1f12192105d14ff5a5462c5103634358": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb6db24a95cb491ebbdd63fc8ca65e47": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb4ec5f670f540189dbc53b4a7ea9e92": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e6649a0b9fa245a6979b8a9f9191dab3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca935e7832f243ee83375d583a6fdd72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0cc47d8b43da45a48670da5f2b927ceb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e45cadfcdb4b4553be995da6a1948df7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4b1013b5331d435c8305ed94d339d93b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_aaa421b58d544e4c819b70b1f1d85c6a",
              "IPY_MODEL_aae595d470ad41a1be2614461c8da1ea",
              "IPY_MODEL_557c1e1f7f4c42a4963ef5fa85f7f688"
            ],
            "layout": "IPY_MODEL_9d52ba3fd559411895504ee5acaed033"
          }
        },
        "aaa421b58d544e4c819b70b1f1d85c6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_91cf3d615c5f465189da164442d0719d",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_879b85eeb73a4ac284fb54756de0b33b",
            "value": "Map:‚Äá100%"
          }
        },
        "aae595d470ad41a1be2614461c8da1ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2723c2ba1e304f478d839691cc65cd41",
            "max": 152,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7898b96fc35e4353abccefb6452f1d7d",
            "value": 152
          }
        },
        "557c1e1f7f4c42a4963ef5fa85f7f688": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf44a0b975e8426d912358be23d9a541",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_01c4448256704ddf998d898a4217aef7",
            "value": "‚Äá152/152‚Äá[00:01&lt;00:00,‚Äá118.84‚Äáexamples/s]"
          }
        },
        "9d52ba3fd559411895504ee5acaed033": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "91cf3d615c5f465189da164442d0719d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "879b85eeb73a4ac284fb54756de0b33b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2723c2ba1e304f478d839691cc65cd41": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7898b96fc35e4353abccefb6452f1d7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bf44a0b975e8426d912358be23d9a541": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "01c4448256704ddf998d898a4217aef7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}